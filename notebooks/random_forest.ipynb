{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF1ANQ/VVgXwHWX0ZMXFlv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magnolia2001/Forest_Estimation/blob/main/notebooks/random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "asg7_6UK-6M_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd660d4-004e-46c0-b20c-458f50908833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 检查挂载的路径结构\n",
        "!ls /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = '/content/drive/My Drive/data/'\n",
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'"
      ],
      "metadata": {
        "id": "bbLCsXAa_EOG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime, os, cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
        "\n",
        "\n",
        "# 制作标签数据和特征数据\n",
        "\n",
        "# 窗口大小应为奇数，以保证标签在中间\n",
        "size = 5 #define window size should be odd so that the label is in the middle\n",
        "# 特征的形状，这里假设每个特征是一个大小为 (size, size) 的窗口，包含 11 个通道\n",
        "shape = (11, size, size) #define shape of features\n",
        "# np.ones(shape, dtype=None) 用于创建一个形状为 shape 的数组，并将所有元素初始化为 1.0\n",
        "# 其中 shape：指定数组的形状，通常是一个整数或元组。 dtype：指定数组元素的数据类型（可选）。如果不指定，默认使用 float64 类型。\n",
        "# labels1 用于存放标签数据（掩膜）, data1 用于存放提取的特征数据\n",
        "# np.ones(1)返回的是一个只有一个元素的数组，其中该元素值为 1。\n",
        "labels1 = np.ones(1) #array for labels\n",
        "# 创建了一个数组，形状为 shape 即 (11, 5, 5) 的 NumPy 数组，并且所有的元素值都被初始化为 1.0 。\n",
        "data1 = np.ones(shape) #array for features\n",
        "# 扩展维度，便于后续拼接操作\n",
        "data1 = np.expand_dims(data1, axis=0) #expand dimension to concatenate\n",
        "\n",
        "# 遍历目录中的图像（假设有 20 张图像, 具体数量还需要根据自己的情况修改）\n",
        "# 在 for j in range(20) 这个遍历过程中，区分 j < 10 和 j >= 10 的目的是为了处理不同的文件命名规则。\n",
        "# 对于小于 10 的文件名，文件名是 \"image_00X.npy\"，其中 X 是单个数字（0 到 9）。\n",
        "# 对于大于等于 10 的文件名，文件名是 \"image_0XY.npy\"，其中 XY 是两位数的数字（10 到 19）。\n",
        "for j in range(142): #iterate over images in directory\n",
        "  if j < 10:\n",
        "    # 路径填写实际路径\n",
        "    # 读取图像数据\n",
        "    X = np.load(f'{path_images}image_00'+ str(j) + '.npy')\n",
        "    # 读取掩膜数据\n",
        "    y = np.load(f'{path_masks}mask_00'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "\n",
        "    # 选择掩膜图像 y 中所有大于 0 的位置（即标签不为 0 的位置），并返回这些位置的索引。\n",
        "    # indices 数组返回 N 个元素，其中 N 为掩膜图像 y 中所有大于 0 的元素个数。每个元素都是一个长度为 2 的行向量，表示符合条件元素的行列索引。\n",
        "    # y > 0 是一个布尔条件，返回一个与 y 相同形状的布尔数组, 如果是大于 0，布尔值为 True，否则为 False。\n",
        "    # np.argwhere() 是 NumPy 库中的一个函数，它返回数组中满足某个条件的所有索引（行列坐标），即满足条件的元素的坐标位置。\n",
        "    # 这里 indices 是一个形状为 (N, 2) 的二维数组，每一行是 (y, x) 坐标. y 为行索引, x 为列索引\n",
        "    indices = np.argwhere(y > 0) #select all values with label\n",
        "\n",
        "    # indices_2d 是 indices 数组的一个切片，是一个 二维数组, 它包含了所有掩膜图像中标签值大于 0 的位置的 列索引。\n",
        "    # 切片操作 indices[:, 1:] 就是提取所有行中的第二列（即 行 和 列 坐标中的 列索引）。\n",
        "    # indices_2d = indices[:, 1:] #extract indices\n",
        "\n",
        "    # 初始化一个数组 ind_y 来收集符合条件的标签位置。\n",
        "    # np.ones(2) 会创建一个包含 2 个元素的数组，所有元素的值为 1. 。\n",
        "    # .reshape(-1, 2) 将该数组的形状重塑为 (-1, 2)，表示按列数为 2 进行重塑，-1 表示自动计算行数。由于只有 2 个元素，这会将数组变成形状为 (1, 2) 的二维数组。\n",
        "    ind_y = np.ones(2).reshape(-1,2) #array to collect indices\n",
        "\n",
        "    # 遍历掩膜中的每个标签位置\n",
        "    # for i in indices_2d: #iterate over indices\n",
        "    for i in indices:\n",
        "      # 提取图像块，并检查其形状。\n",
        "      # size//2 表示 size 除以 2 的整数部分，用于确定图像块中心点到边界的距离。右端点的值之所以加 1 是因为区间是左闭右开的,所以加 1 保证能取到右端点.\n",
        "      # 整个i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2)表达式计算出一个范围，用于选取以 (i[0], i[1])（标签的 y, x 坐标） 为中心，上下各延伸 size//2 个像素的区域。\n",
        "      # i[0] 是当前标签位置的 y 坐标（行索引）。i[1] 是当前标签位置的 x 坐标（列索引）。\n",
        "      # 利用 shape 确保当前窗口大小与指定的窗口大小一致\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape: #select only features with the same shape because of labels at the image border\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1] #save them temporary\n",
        "        # 将 temp 的维度扩展一个维度，使得它变成一个形状为 (1, channels, size, size) 的四维数组。扩展维度的目的是为了能够将 temp 与其他提取的窗口进行拼接。\n",
        "        temp2 = np.expand_dims(temp, axis=0) #expand dimension to concatenate\n",
        "        # 拼接特征数据\n",
        "        # data1 最终会变成 (num_samples, 11, 5, 5)，其中 num_samples 是提取的窗口数量。\n",
        "        data1 = np.concatenate((data1, temp2), axis=0) #concatenation\n",
        "        # 拼接标签索引\n",
        "        # i.reshape(-1, 2) 会把 i 重新调整为一个形状为 (1, 2) 的二维数组\n",
        "        # axis=0 表示在 第 0 维（行方向） 进行拼接，即新添加的行会被添加到原数组的最后。\n",
        "        # ind_y 则是一个 (num_samples, 2) 的数组，每个样本对应一个标签位置的 (y, x) 坐标。\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0) #concatenation of index so that they have the same order and length as the features\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    # 初始时，ind_y 中的第一个元素是 np.ones(2).reshape(-1,2) 创建的虚拟数据。此步骤是将它移除，只保留实际的标签坐标。\n",
        "    ind_y = ind_y[1:] #remove first dummy values\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # 提取标签值\n",
        "    # 从这句代码应该可以看出原作者的掩膜图像的形状包含了通道维度,即形状为(1, height, width)\n",
        "    # data_y = y[0, indices_1, indices_2] #extract labels\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0) #concatenate labels\n",
        "\n",
        "  if j >= 10 and j < 100:\n",
        "    X = np.load(f'{path_images}image_0'+ str(j) + '.npy')\n",
        "    y = np.load(f'{path_masks}mask_0'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "    indices = np.argwhere(y > 0)\n",
        "    # indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    # for i in indices_2d:\n",
        "    for i in indices:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    ind_y = ind_y[1:]\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # data_y = y[0, indices_1, indices_2]\n",
        "    # 提取标签值\n",
        "    # 每一对 (indices_1[i], indices_2[i]) 会自动匹配，得到对应位置的标签值。\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "  if j >= 100:\n",
        "    X = np.load(f'{path_images}image_'+ str(j) + '.npy')\n",
        "    y = np.load(f'{path_masks}mask_'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "    indices = np.argwhere(y > 0)\n",
        "    # indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    # for i in indices_2d:\n",
        "    for i in indices:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    ind_y = ind_y[1:]\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # data_y = y[0, indices_1, indices_2]\n",
        "    # 提取标签值\n",
        "    # 每一对 (indices_1[i], indices_2[i]) 会自动匹配，得到对应位置的标签值。\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "# 移除第一个虚拟值\n",
        "# data1 的形状会是 (num_samples, 11, 5, 5)，其中 num_samples 是提取的窗口数量（即符合条件的标签数量）。一个四维数组\n",
        "# labels1 的形状会是 (num_samples,)，其中 num_samples 是所有图像中符合条件的标签数量。一个一维数组\n",
        "data1 = data1[1:] #remove first dummy values\n",
        "labels1 = labels1[1:] #remove first dummy values\n",
        "\n",
        "# data1 和 labels1 应该是 一一对应的，因为它们的样本数（num_samples）相同。\n",
        "# 获得标签数据和特征数据\n",
        "features = data1\n",
        "labels = labels1\n"
      ],
      "metadata": {
        "id": "El4NkzMi_ExF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 每个类别的样本数，确保每个标签区间有 800 个样本\n",
        "# sample_size = 800 #every class with labels smaller 36 meters has over 800 values\n",
        "# #features = np.mean(features, axis=(2, 3)) # patch mean of size * size features\n",
        "\n",
        "# # 生成从 3 到 36 步长为 3 的数字列表，即 [3, 6, 9, ..., 36]\n",
        "# num = (list(range(3, 37, 3))) #create list from 3 to 36 step 3\n",
        "# # 假设每个样本是一个 5x5 的图像块（大小为 5x5，90 个通道）\n",
        "# shape = (11, 5, 5)\n",
        "# # 创建一个初始的数组用于存储特征数据，形状为 (90, 5, 5)\n",
        "# data_bal = np.ones(shape) #create array to fill with features\n",
        "# # 扩展维度，使得形状变为 (1, 11, 5, 5)，这样可以进行拼接\n",
        "# data_bal = np.expand_dims(data_bal, axis=0) #expand one dimension to concatenate\n",
        "# # 创建一个用于存储标签的初始数组，形状为 (1,)\n",
        "# data_lab = np.ones(1) #create array to fill labels\n",
        "\n",
        "# # 在抽样之前，打印每个区间的样本数量，确保逻辑合理。\n",
        "# # 遍历每个标签区间\n",
        "# for i in num:\n",
        "#   # 注意这个 i 是区间右端点\n",
        "#   # 从标签中选择属于当前区间的索引\n",
        "#   # np.where() 返回的是一个元组，元组的元素个数取决于判断条件中的数据的维度, 元组的每个元素都是一个 数组，这些数组表示满足条件的元素在原始数组中的索引。因此，需要通过 indices[0] 访问索引数组\n",
        "#   # 如果输入数组是 多维的，返回的元组会包含 每一维的索引数组。例如，若数组是三维的，返回的元组就会包含三个数组，分别表示满足条件的元素在三维空间中每一维的索引。\n",
        "#   indices = np.where((labels > i-3) & (labels <= i)) #select indcies from every 3 meter interval until 36\n",
        "#   print(f\"区间 ({i-3}, {i}] 的样本数: {len(indices[0])}\")\n",
        "\n",
        "#   # 根据样本数决定如何抽样\n",
        "#   if len(indices[0]) < sample_size:\n",
        "#       print(f\"样本不足800，仅有 {len(indices[0])} 个样本，允许重复抽样。\")\n",
        "#       sampled_indices = np.random.choice(indices[0], size=sample_size, replace=True)\n",
        "#   else:\n",
        "#       sampled_indices = np.random.choice(indices[0], size=sample_size, replace=False)\n",
        "\n",
        "\n",
        "#   # 在当前区间中随机抽样 800 个样本\n",
        "#   # sampled_indices = np.random.choice(indices[0].flatten(), size=sample_size, replace=False) #random sample of each interval\n",
        "#   # sampled_indices = np.random.choice(indices[0], size=sample_size, replace=False)\n",
        "\n",
        "#   # 提取对应的特征和标签\n",
        "#   tempx = features[sampled_indices]\n",
        "#   tempy = labels[sampled_indices]\n",
        "#   # 将当前区间的特征和标签拼接到平衡数组中\n",
        "#   data_bal = np.concatenate((data_bal, tempx), axis=0)\n",
        "#   data_lab = np.concatenate((data_lab, tempy), axis=0)\n",
        "\n",
        "# # 处理 labels > 36 的标签，这部分直接拼接\n",
        "# indices = np.where((labels > 36)) #add the values > 36 m, they are so few no sample needed\n",
        "# # sampled_indices = indices[0].flatten()\n",
        "# sampled_indices = indices[0]\n",
        "# tempx = features[sampled_indices]\n",
        "# tempy = labels[sampled_indices]\n",
        "# data_bal = np.concatenate((data_bal[1:], tempx), axis=0)\n",
        "# data_lab = np.concatenate((data_lab[1:], tempy), axis=0)\n",
        "\n",
        "# # # 为了配合可视化界面，这里将data_bal重新赋值给features将data_lab重新赋值给labels，以便后续能够使用统一的变量。\n",
        "# features = data_bal\n",
        "# labels = data_lab\n"
      ],
      "metadata": {
        "id": "-kJxDrQP_Nnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# random_forest"
      ],
      "metadata": {
        "id": "1H6WWNZK_R8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据集拆分：训练集和测试集\n",
        "# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=3) #create train, test set\n",
        "\n",
        "# features 数组形状为 (num_samples, 11, 5, 5)，意味着每个样本有 11 个特征（或 11 个通道），每个特征是一个 5x5 的空间窗口。\n",
        "features_mean = np.mean(features, axis=(2, 3)) # patch mean of size * size features\n",
        "\n",
        "# 现在 features_mean 的形状是 (num_samples, 11)，适用于NN或者其他传统机器学习算法\n",
        "# 注意: train_test_split 只能处理 NumPy 数组或 Pandas DataFrame，并不能直接处理 TensorFlow Dataset 对象。因此，这部分代码在处理 TensorFlow Dataset 时会出错。\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_mean, labels, test_size = 0.3, random_state=3)\n"
      ],
      "metadata": {
        "id": "Ny0lckRuAQVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Search**"
      ],
      "metadata": {
        "id": "allfxVTUAKre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 用于保存机器学习模型到指定路径，以便后续可以重新加载并使用，而不需要重新训练模型。\n",
        "def save_model(model, modelname):\n",
        "    \"\"\"\n",
        "    保存机器学习模型。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: sklearn.ensemble.*\n",
        "      训练好的机器学习模型。\n",
        "    model_name: String\n",
        "      模型名称。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the models directory exists\n",
        "    os.makedirs('/content/drive/My Drive/forest_height/models/', exist_ok=True)\n",
        "\n",
        "    # Save the model\n",
        "    # 始终希望启用压缩，可以直接在 joblib.dump 调用中设置 compress。\n",
        "    # joblib.dump 函数支持 compress 参数，用于对保存的模型文件进行压缩。启用压缩不会破坏模型，只是减小文件大小。\n",
        "    # 模型保存时的压缩不会影响模型加载，加载时不需要指定任何参数。\n",
        "    joblib.dump(model, f'/content/drive/My Drive/forest_height/models/{modelname}.joblib', compress=True)\n",
        "\n",
        "    print(f\"Model saved as '/content/drive/My Drive/forest_height/models/{modelname}.joblib'\")\n",
        "\n",
        "    # 加载保存的模型。加载的模型对象与保存前完全一致，可以直接用于推理或评估，无需重新训练。将其单独定义为一个函数更加方便。\n",
        "    # load model with:\n",
        "    # model = joblib.load(\"forest_height/models/{model_name}.joblib\")\n",
        "\n",
        "\n",
        "\n",
        "# load_model 函数返回的模型与save_model(model, modelname)保存的模型完全一致\n",
        "def load_model(modelname):\n",
        "    \"\"\"\n",
        "    Load a previously saved model from a .joblib file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    modelname : str\n",
        "        The name of the saved model file (without extension).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : object\n",
        "        The loaded machine learning model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the path to the model\n",
        "    model_path = f'/content/drive/My Drive/forest_height/models/{modelname}.joblib'\n",
        "\n",
        "    # Check if the model file exists\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model file '{model_path}' does not exist.\")\n",
        "\n",
        "    # Load and return the model\n",
        "    model = joblib.load(model_path)\n",
        "    print(f\"Model loaded from '{model_path}'\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Nk5_9bCsBBku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 为了适应不同的数据集，cols 应该动态调整，确保其与数据集的特征对应。\n",
        "def feature_importance(\n",
        "    model,\n",
        "    model_name,\n",
        "    columns = ['Height', 'HH_Dir4_Mean', 'HV_Dir3_Mean', 'HV_Dir4_Mean', 'Entropy', 'mv', 'RLD12', 'RLD20', 'RLD5', 'sigma_db_HV', 'SinAspect']\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Visualize feature importance of regression model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: sklearn.ensemble.*\n",
        "      一个已经训练好的回归模型，必须具有 feature_importances_ 属性。\n",
        "    model_name: String\n",
        "      模型的名称，用于可视化标题中显示。\n",
        "    cols: Array of Strings\n",
        "      list（字符串数组）.特征名称列表，用于匹配模型中的特征顺序。如果为 None，则自动生成 [\"Feature 1\", \"Feature 2\", ...]。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None, just prints out feature importances and plots them in a bar graph\n",
        "      直接打印和绘制特征重要性。\n",
        "    \"\"\"\n",
        "    # 这是一个数组，存储模型中每个特征的重要性分数。\n",
        "    # importance = model.feature_importances_ 这个数组的长度与为喂给model进行训练模型的数据集中的特征数量一致，即importance 的长度等于训练数据集中特征的数量。\n",
        "    importance = model.feature_importances_\n",
        "\n",
        "    # summarize feature importance\n",
        "    # 打印特征重要性\n",
        "    # enumerate(importance) 枚举特征重要性数组，i 是特征索引，v 是对应的重要性分数\n",
        "    for i,v in enumerate(importance):\n",
        "        # 简单地用特征索引（Feature: 0、Feature: 1 等）来表示特征。\n",
        "        print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "\n",
        "    # 如果没有传入 cols，则动态生成\n",
        "    if cols is None:\n",
        "        cols = [f\"Feature {i}\" for i in range(len(importance))]\n",
        "    elif len(cols) != len(importance):\n",
        "        raise ValueError(\"Length of 'cols' does not match number of features in the model.\")\n",
        "\n",
        "    # plot feature importance\n",
        "    # 将图像宽度适当拉长，使其能够容纳更多的特征名称。\n",
        "    # plt.figure(figsize=(20, 10))  # 宽20，高10\n",
        "    # cols：x 轴的特征名称（列表）。  importance：y 轴的特征重要性分数。  color=color：条形图的颜色（需要外部定义 color，否则会报错）。\n",
        "    plt.bar(cols, importance, color=\"#01748F\")\n",
        "    # 设置 x 轴标签\n",
        "    plt.xlabel(\"Features\")\n",
        "    # 设置 y 轴标签\n",
        "    plt.ylabel(\"Feature Importance\")\n",
        "    # 图表标题，显示模型名称\n",
        "    plt.title(f\"Feature Importance of {model_name} Regression\")\n",
        "    plt.xticks(rotation=45)  # 调整标签角度，避免标签之间的重叠\n",
        "    # 显示图表\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Pp8DONKvH3Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pred_vs_true(model, model_name):\n",
        "    \"\"\"\n",
        "    Visualize predictions and compare them to the labeled data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: sklearn.ensemble.*\n",
        "      训练好的机器学习模型，用于预测。通过 model.predict(X_test) 生成预测值。\n",
        "    model_name: String\n",
        "      字符串，表示模型名称，用于可视化时的标题显示。\n",
        "\n",
        "    该函数不输入数据集，直接使用在前面代码中划分的数据集数据集即可。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None, just prints out errors of each dataset\n",
        "      该函数没有返回值，仅通过两种可视化方式展示预测值和真实值的关系：\n",
        "      1.整体预测值 vs. 真实值的散点图。点为蓝色点。展示模型整体性能：预测值和真实值是否接近对角线。\n",
        "      2.单一通道（特征） vs. 森林高度的散点图。黑色点为真实值，蓝色点为预测值。分别展示两个特定通道（第四通道和第五通道）特征与森林高度（真实值和预测值）的关系。帮助分析模型是否在这些特定特征通道上表现良好。\n",
        "    \"\"\"\n",
        "    # Get model predictions\n",
        "    # 注意这是训练好保存下来的机器学习模型，用于预测\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # visualize predictions vs. true labels\n",
        "    # 可视化 1 - 整体预测值 vs. 真实值\n",
        "    fig = plt.figure(figsize=(6,6))\n",
        "    # 绘制 y_pred（预测值）和 y_test（真实值）的散点图。\n",
        "    # color=color 控制点的颜色（需要外部定义），alpha=0.5 设置点的透明度。\n",
        "    plt.scatter(y_pred, y_test, color=\"#01748F\", alpha=0.5)\n",
        "    plt.xticks(rotation=45)\n",
        "    # 坐标轴格式化\n",
        "    # 对 x 轴刻度值进行格式化为无小数点的整数。\n",
        "    plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
        "    # 添加对角线\n",
        "    # 绘制对角线（黑色虚线），表示理想状态下预测值等于真实值（y_pred = y_test）。\n",
        "    plt.plot([-1,75], [-1, 75], 'k--')\n",
        "    # 坐标轴设置\n",
        "    plt.xlabel(\"Predictions\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    # 设置 x 和 y 轴的范围（硬编码）\n",
        "    plt.xlim([-1,75])\n",
        "    plt.ylim([-1,75])\n",
        "    # 根据模型名称动态生成标题\n",
        "    plt.title(f\"{model_name} Regression: Prediction vs. Labels\")\n",
        "    # 显示图像\n",
        "    plt.show()\n",
        "\n",
        "    # 可视化 2 - 单一特征 vs. 森林高度\n",
        "    # 实现绘制两幅图：\n",
        "    # 一幅是HH极化通道的sigmadB（绘制在X轴上）与真实值 y_test 的散点图；\n",
        "    # 另一幅是HV极化通道的sigmadB（绘制在X轴上）与真实值 y_test 的散点图\n",
        "    # 因为太高维的数据对人类来说是无法可视化的\n",
        "\n",
        "    # Part 2: Visualize ninth channel vs. forest height\n",
        "    # 绘制 X_test 的第 9 通道值（HV极化通道的sigmadB）与真实值 y_test 的散点图，颜色为黑色，点大小为 10。\n",
        "    fig, ax = plt.subplots()\n",
        "    # 选择特定特征通道，绘制散点图。\n",
        "    # 提取 X_test 中的第四通道（索引从 0 开始，第 4 通道为索引 3）\n",
        "    plt.scatter(X_test[:,9], y_test, 10, color='black')  # Fourth channel vs. true labels\n",
        "    plt.scatter(X_test[:,9], y_pred, 10, color=\"#01748F\")  # Fourth channel vs. predictions\n",
        "    # 标题\n",
        "    plt.title(f'{model_name} Regression: Sigma0_db_HH and Forest Height')\n",
        "    # 轴\n",
        "    plt.xlabel('Sigma0_db_HH')\n",
        "    plt.ylabel('Forest Height')\n",
        "    # 图例\n",
        "    # 添加图例，标识黑色点为真实值，其他颜色点为预测值。\n",
        "    ax.legend((\"True Value\", \"Prediction\"), loc='upper left')\n",
        "    # 显示图像\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "cuPz5LeHLp5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, test_features, test_labels):\n",
        "    \"\"\"\n",
        "    Evaluate a model on specified datasets and return comprehensive evaluation metrics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: sklearn.ensemble.*\n",
        "      A trained model instance (e.g., RandomForestRegressor).\n",
        "    test_features: numpy.ndarray\n",
        "      Test features (X_test), usually a 2D array or matrix.\n",
        "    test_labels: numpy.ndarray\n",
        "      Test labels (y_test), usually a 1D array or vector representing true values.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      A dictionary containing evaluation metrics:\n",
        "      - errors: Absolute errors for each sample (numpy.ndarray)\n",
        "      - mae: Mean Absolute Error (float)\n",
        "      - mse: Mean Squared Error (float)\n",
        "      - rmse: Root Mean Squared Error (float)\n",
        "      - mape: Mean Absolute Percentage Error (float)\n",
        "      - r2: Coefficient of determination (R²) (float)\n",
        "      - accuracy: Model accuracy in percentage (float)\n",
        "    \"\"\"\n",
        "    # 模型预测\n",
        "    predictions = model.predict(test_features)\n",
        "\n",
        "    # 绝对误差\n",
        "    errors = abs(predictions - test_labels)\n",
        "\n",
        "    # 计算 MAE, MSE, RMSE, 和 MAPE\n",
        "    mae = mean_absolute_error(test_labels, predictions)\n",
        "    mse = mean_squared_error(test_labels, predictions)\n",
        "    rmse = mse ** 0.5\n",
        "    mape = mean_absolute_percentage_error(test_labels, predictions) * 100\n",
        "\n",
        "    # 计算 R²\n",
        "    r2 = r2_score(test_labels, predictions)\n",
        "\n",
        "    # 计算准确率 (Accuracy = 100 - MAPE)\n",
        "    accuracy = 100 - mape\n",
        "\n",
        "    # 打印模型评估结果\n",
        "    # 通过访问返回的字典的键，可以获取每个指标的值或者将字典中的值分别赋值给单独的变量。比如：假设 model, X_test, y_test 是已经定义的\n",
        "    # results = evaluate_model_performance(model, X_test, y_test)\n",
        "    # print(\"Errors:\", results['errors'])\n",
        "    # errors = results['errors']\n",
        "    print('Model Performance:')\n",
        "    print('Average Error (Absolute): {:0.4f}'.format(np.mean(errors)))\n",
        "    print('MAE: {:0.4f}'.format(mae))\n",
        "    print('MSE: {:0.4f}'.format(mse))\n",
        "    print('RMSE: {:0.4f}'.format(rmse))\n",
        "    print('MAPE: {:0.2f}%'.format(mape))\n",
        "    print('R²: {:0.4f}'.format(r2))\n",
        "    print('Accuracy: {:0.2f}%'.format(accuracy))\n",
        "\n",
        "    # 返回包含所有指标的字典\n",
        "    return {\n",
        "        'errors': errors,\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mape': mape,\n",
        "        'r2': r2,\n",
        "        'accuracy': accuracy\n",
        "    }\n"
      ],
      "metadata": {
        "id": "5B5cA05fNenU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def normalize_color_np(img):\n",
        "    \"\"\"\n",
        "    Normalize a multi-band image for visualization (RGB channels).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img : numpy.ndarray\n",
        "        Multi-band image with shape (color_channels, height, width).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "        RGB image with shape (height, width, 3), normalized to [0, 1].\n",
        "    \"\"\"\n",
        "    # 检查输入是否为 3D 图像\n",
        "    assert len(img.shape) == 3, \"Input X must have 3 dimensions (color_channels, height, width).\"\n",
        "\n",
        "    # 提取红色、绿色和蓝色通道（可以根据需求选择不同的通道）\n",
        "    # 提取的 red, green, 和 blue 通道：形状均为 (height, width)\n",
        "    red = img[0, :, :]  # 第 13 通道\n",
        "    green = img[1, :, :]  # 第 14 通道\n",
        "    blue = img[2, :, :]  # 第 15 通道\n",
        "\n",
        "    # 对各通道进行归一化到 [0, 1]\n",
        "    # 归一化后的 red_norm, green_norm, 和 blue_norm：归一化不会改变数组的形状，仍然是 (height, width)。\n",
        "    red_norm = (red - red.min()) / (red.max() - red.min())\n",
        "    green_norm = (green - green.min()) / (green.max() - green.min())\n",
        "    blue_norm = (blue - blue.min()) / (blue.max() - blue.min())\n",
        "\n",
        "    # 合并为 RGB 图像\n",
        "    # axis=-1 表示将输入数组沿新轴堆叠到最后一个维度。堆叠结果将生成一个新的 3D 数组，形状为 (height, width, 3)，即每个像素点对应一个 RGB 值。\n",
        "    return np.stack((red_norm, green_norm, blue_norm), axis=-1)\n",
        "\n",
        "\n",
        "def plot_img_np(img):\n",
        "    \"\"\"\n",
        "    Visualize a single image (either multi-band X or single-band prediction).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img : numpy.ndarray\n",
        "        Input image. Can be:\n",
        "        - A multi-band satellite image (shape: (color_channels, height, width)).\n",
        "        - A single-band prediction (shape: (height, width)).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # 判断输入是否为 3D 图像\n",
        "    if len(img.shape) == 3:  # 多波段图像\n",
        "        img = normalize_color_np(img)  # 调用 normalize_color 进行归一化并转换为 RGB 图像\n",
        "\n",
        "    # 绘制图像\n",
        "    plt.figure(figsize=(6, 6))  # 设置画布大小\n",
        "    plt.imshow(img, cmap='viridis' if len(img.shape) == 2 else None)  # 单波段使用色彩映射，RGB 图像直接显示\n",
        "    plt.colorbar() if len(img.shape) == 2 else None  # 单波段图像显示 colorbar\n",
        "    plt.axis(\"off\")  # 关闭坐标轴\n",
        "    plt.show() # 显示窗口"
      ],
      "metadata": {
        "id": "ieSjXSvBSTCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [100, 200, 500] #[int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt', 'log2']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10, 15]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'criterion': ['mse', 'mae'],\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "\n",
        "%%time\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# initialize model\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "rf_random = RandomizedSearchCV(\n",
        "    estimator = rf,\n",
        "    param_distributions = random_grid,\n",
        "    # scoring=\"neg_mean_absolute_error\", # strategy to evaluate the performance\n",
        "    n_iter = 150,\n",
        "    cv = 5, # k-fold cross-validation\n",
        "    verbose=2, # the higher, the more messages\n",
        "    random_state=3,\n",
        "    #n_jobs=-1, # use all processors\n",
        "    n_jobs = -1,\n",
        "    return_train_score=True)\n",
        "\n",
        "# train model\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# 将会打印出最佳超参数\n",
        "rf_random.best_params_\n",
        "\n",
        "# 保存模型\n",
        "import joblib\n",
        "save_model(rf_random, \"random_forest\")\n",
        "\n",
        "# 保存模型和加载模型时，模型名称都无后缀\n",
        "rf = load_model(\"random_forest\")\n",
        "\n",
        "# 打印特征及其重要性,以及特征重要性可视化\n",
        "feature_importance(rf, \"random_forest\")\n",
        "\n",
        "# 输出两类图,\n",
        "# 一为 整体预测值 vs. 真实值的散点图,点为蓝色点。\n",
        "# 二为 单一特征 vs. 森林高度。黑色点为真实值，蓝色点为预测值。分别展示两个特定通道（第四通道和第五通道）特征与森林高度（真实值和预测值）的关系。\n",
        "pred_vs_true(rf, \"random_forest\")\n",
        "\n",
        "# 直接将函数返回的字典赋值给一个变量。\n",
        "results = evaluate_model(rf, X_test, y_test)\n",
        "# 通过访问字典的键，可以获取每个指标的值。\n",
        "errors = results['errors']\n",
        "mae = results['mae']\n",
        "mse = results['mse']\n",
        "rmse = results['rmse']\n",
        "mape = results['mape']\n",
        "r2 = results['r2']\n",
        "accuracy = results['accuracy']\n"
      ],
      "metadata": {
        "id": "QYzkoimR_QcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可视化预测结果\n",
        "img = rf.predict(X_test)\n",
        "plot_img_np(img)"
      ],
      "metadata": {
        "id": "4WG5wtXwBLK4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoaTuYGlI+/SDg2k0daMWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magnolia2001/Forest_Estimation/blob/main/notebooks/random_forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "asg7_6UK-6M_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a891b1-cdce-4bce-e457-4042b4ad2e20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 检查挂载的路径结构\n",
        "!ls /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = '/content/drive/My Drive/data/'\n",
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'"
      ],
      "metadata": {
        "id": "bbLCsXAa_EOG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "在生成栅格切片和掩膜切片的代码中，输出的 .npy 文件的数据结构应为：\n",
        "\n",
        "print(\"Image shape:\", image.shape)  # 应为 (11, 1024, 1024)  # 应为 (color_channels, height, width)\n",
        "\n",
        "print(\"Mask shape:\", mask.shape)    # 应为 (1024, 1024)  # 应为 (height, width)"
      ],
      "metadata": {
        "id": "VZkSQrhP0Q1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime, os, cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
        "\n",
        "\n",
        "# 制作标签数据和特征数据\n",
        "\n",
        "# 窗口大小应为奇数，以保证标签在中间\n",
        "size = 5 #define window size should be odd so that the label is in the middle\n",
        "# 特征的形状，这里假设每个特征是一个大小为 (size, size) 的窗口，包含 11 个通道\n",
        "shape = (11, size, size) #define shape of features\n",
        "# np.ones(shape, dtype=None) 用于创建一个形状为 shape 的数组，并将所有元素初始化为 1.0\n",
        "# 其中 shape：指定数组的形状，通常是一个整数或元组。 dtype：指定数组元素的数据类型（可选）。如果不指定，默认使用 float64 类型。\n",
        "# labels1 用于存放标签数据（掩膜）, data1 用于存放提取的特征数据\n",
        "# np.ones(1)返回的是一个只有一个元素的数组，其中该元素值为 1。\n",
        "labels1 = np.ones(1) #array for labels\n",
        "# 创建了一个数组，形状为 shape 即 (11, 5, 5) 的 NumPy 数组，并且所有的元素值都被初始化为 1.0 。\n",
        "data1 = np.ones(shape) #array for features\n",
        "# 扩展维度，便于后续拼接操作\n",
        "data1 = np.expand_dims(data1, axis=0) #expand dimension to concatenate\n",
        "\n",
        "# 初始化所有索引，初始化为一个空 NumPy 数组\n",
        "indices_all = np.empty((0, 2), dtype=int)  # 累积所有索引\n",
        "\n",
        "# 遍历目录中的图像（假设有 20 张图像, 具体数量还需要根据自己的情况修改）\n",
        "# 在 for j in range(20) 这个遍历过程中，区分 j < 10 和 j >= 10 的目的是为了处理不同的文件命名规则。\n",
        "# 对于小于 10 的文件名，文件名是 \"image_00X.npy\"，其中 X 是单个数字（0 到 9）。\n",
        "# 对于大于等于 10 的文件名，文件名是 \"image_0XY.npy\"，其中 XY 是两位数的数字（10 到 19）。\n",
        "for j in range(142): #iterate over images in directory\n",
        "  if j < 10:\n",
        "    # 路径填写实际路径\n",
        "    # 读取图像数据\n",
        "    X = np.load(f'{path_images}image_00'+ str(j) + '.npy')\n",
        "    # 读取掩膜数据\n",
        "    y = np.load(f'{path_masks}mask_00'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "\n",
        "    # 选择掩膜图像 y 中所有大于 0 的位置（即标签不为 0 的位置），并返回这些位置的索引。\n",
        "    # indices 数组返回 N 个元素，其中 N 为掩膜图像 y 中所有大于 0 的元素个数。每个元素都是一个长度为 2 的行向量，表示符合条件元素的行列索引。\n",
        "    # y > 0 是一个布尔条件，返回一个与 y 相同形状的布尔数组, 如果是大于 0，布尔值为 True，否则为 False。\n",
        "    # np.argwhere() 是 NumPy 库中的一个函数，它返回数组中满足某个条件的所有索引（行列坐标），即满足条件的元素的坐标位置。\n",
        "    # 这里 indices 是一个形状为 (N, 2) 的二维数组，每一行是 (y, x) 坐标. y 为行索引, x 为列索引\n",
        "    # indices = np.argwhere(y > 0) #select all values with label\n",
        "    indices = np.argwhere((y > 0) & (y < 127.58))\n",
        "\n",
        "    # indices_2d 是 indices 数组的一个切片，是一个 二维数组, 它包含了所有掩膜图像中标签值大于 0 的位置的 列索引。\n",
        "    # 切片操作 indices[:, 1:] 就是提取所有行中的第二列（即 行 和 列 坐标中的 列索引）。\n",
        "    # indices_2d = indices[:, 1:] #extract indices\n",
        "\n",
        "    # 初始化一个数组 ind_y 来收集符合条件的标签位置。\n",
        "    # np.ones(2) 会创建一个包含 2 个元素的数组，所有元素的值为 1. 。\n",
        "    # .reshape(-1, 2) 将该数组的形状重塑为 (-1, 2)，表示按列数为 2 进行重塑，-1 表示自动计算行数。由于只有 2 个元素，这会将数组变成形状为 (1, 2) 的二维数组。\n",
        "    ind_y = np.ones(2).reshape(-1,2) #array to collect indices\n",
        "\n",
        "    # 遍历掩膜中的每个标签位置\n",
        "    # for i in indices_2d: #iterate over indices\n",
        "    for i in indices:\n",
        "      # 提取图像块，并检查其形状。\n",
        "      # size//2 表示 size 除以 2 的整数部分，用于确定图像块中心点到边界的距离。右端点的值之所以加 1 是因为区间是左闭右开的,所以加 1 保证能取到右端点.\n",
        "      # 整个i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2)表达式计算出一个范围，用于选取以 (i[0], i[1])（标签的 y, x 坐标） 为中心，上下各延伸 size//2 个像素的区域。\n",
        "      # i[0] 是当前标签位置的 y 坐标（行索引）。i[1] 是当前标签位置的 x 坐标（列索引）。\n",
        "      # 利用 shape 确保当前窗口大小与指定的窗口大小一致\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape: #select only features with the same shape because of labels at the image border\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1] #save them temporary\n",
        "        # 将 temp 的维度扩展一个维度，使得它变成一个形状为 (1, channels, size, size) 的四维数组。扩展维度的目的是为了能够将 temp 与其他提取的窗口进行拼接。\n",
        "        temp2 = np.expand_dims(temp, axis=0) #expand dimension to concatenate\n",
        "        # 拼接特征数据\n",
        "        # data1 最终会变成 (num_samples, 11, 5, 5)，其中 num_samples 是提取的窗口数量。\n",
        "        data1 = np.concatenate((data1, temp2), axis=0) #concatenation\n",
        "        # 拼接标签索引\n",
        "        # i.reshape(-1, 2) 会把 i 重新调整为一个形状为 (1, 2) 的二维数组\n",
        "        # axis=0 表示在 第 0 维（行方向） 进行拼接，即新添加的行会被添加到原数组的最后。\n",
        "        # ind_y 则是一个 (num_samples, 2) 的数组，每个样本对应一个标签位置的 (y, x) 坐标。\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0) #concatenation of index so that they have the same order and length as the features\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    # 初始时，ind_y 中的第一个元素是 np.ones(2).reshape(-1,2) 创建的虚拟数据。此步骤是将它移除，只保留实际的标签坐标。\n",
        "    ind_y = ind_y[1:] #remove first dummy values\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # 提取标签值\n",
        "    # 从这句代码应该可以看出原作者的掩膜图像的形状包含了通道维度,即形状为(1, height, width)\n",
        "    # data_y = y[0, indices_1, indices_2] #extract labels\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0) #concatenate labels\n",
        "\n",
        "    # 累积所有 .npy 文件中的索引\n",
        "    indices_all = np.vstack((indices_all, ind_y))\n",
        "\n",
        "  if j >= 10 and j < 100:\n",
        "    X = np.load(f'{path_images}image_0'+ str(j) + '.npy')\n",
        "    y = np.load(f'{path_masks}mask_0'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "    # indices = np.argwhere(y > 0)\n",
        "    indices = np.argwhere((y > 0) & (y < 127.58))\n",
        "    # indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    # for i in indices_2d:\n",
        "    for i in indices:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    ind_y = ind_y[1:]\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # data_y = y[0, indices_1, indices_2]\n",
        "    # 提取标签值\n",
        "    # 每一对 (indices_1[i], indices_2[i]) 会自动匹配，得到对应位置的标签值。\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "    # 累积所有 .npy 文件中的索引\n",
        "    indices_all = np.vstack((indices_all, ind_y))\n",
        "\n",
        "  if j >= 100:\n",
        "    X = np.load(f'{path_images}image_'+ str(j) + '.npy')\n",
        "    y = np.load(f'{path_masks}mask_'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "    # indices = np.argwhere(y > 0)\n",
        "    indices = np.argwhere((y > 0) & (y < 127.58))\n",
        "    # indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    # for i in indices_2d:\n",
        "    for i in indices:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    ind_y = ind_y[1:]\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # data_y = y[0, indices_1, indices_2]\n",
        "    # 提取标签值\n",
        "    # 每一对 (indices_1[i], indices_2[i]) 会自动匹配，得到对应位置的标签值。\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "    # 累积所有 .npy 文件中的索引\n",
        "    indices_all = np.vstack((indices_all, ind_y))\n",
        "\n",
        "# 移除第一个虚拟值\n",
        "# data1 的形状会是 (num_samples, 11, 5, 5)，其中 num_samples 是提取的窗口数量（即符合条件的标签数量）。一个四维数组\n",
        "# labels1 的形状会是 (num_samples,)，其中 num_samples 是所有图像中符合条件的标签数量。一个一维数组\n",
        "data1 = data1[1:] #remove first dummy values\n",
        "labels1 = labels1[1:] #remove first dummy values\n",
        "\n",
        "# data1 和 labels1 应该是 一一对应的，因为它们的样本数（num_samples）相同。\n",
        "# 获得标签数据和特征数据\n",
        "features = data1\n",
        "labels = labels1\n"
      ],
      "metadata": {
        "id": "El4NkzMi_ExF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据增强"
      ],
      "metadata": {
        "id": "typUEg_fjW3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. 数据增强管道\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    # TensorFlow 2.5 及以上\n",
        "    # RandomRotation: 随机旋转特征数据，最大旋转角度为 ±40% 的全角。\n",
        "    # 这些增强操作仅应用于特征数据，标签数据保持不变。\n",
        "\n",
        "    # RandomFlip: 随机水平和垂直翻转特征数据。\n",
        "    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    # 在 Python 语法中，列表中最后一个元素后面的逗号是可选的。为了代码风格一致性，建议列表或字典等结构中，最后一行的元素后保持逗号，这样便于以后增加或调整内容\n",
        "    # RandomRotation: 随机旋转特征数据，最大旋转角度为 ±40% 的全角。\n",
        "    keras.layers.RandomRotation(0.4),\n",
        "\n",
        "    # TensorFlow 2.4 及以下\n",
        "    # keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\")\n",
        "    # keras.layers.experimental.preprocessing.RandomRotation(0.4)\n",
        "])\n",
        "\n",
        "# 2. 特征与标签的同步增强\n",
        "def augment_feature_label(feature, label):\n",
        "    \"\"\"\n",
        "    对单个特征和标签进行同步数据增强。\n",
        "    \"\"\"\n",
        "    # features 的形状是 (num_samples, 11, 5, 5)\n",
        "    # labels 的形状是 (num_samples,)\n",
        "    # 对于特征数据，需要将 (11, 5, 5) 转换为 (5, 5, 11) 以便进行数据增强操作。\n",
        "    # 增强后，再将其转换回原来的格式 (11, 5, 5)。\n",
        "\n",
        "    #  1. Keras 数据增强层要求输入形状为 (H, W, C)，特征 feature 的形状从 (11, 5, 5) 转换为 (5, 5, 11)。\n",
        "    feature = tf.transpose(feature, perm=[1, 2, 0])  # (11, 5, 5) -> (5, 5, 11)\n",
        "\n",
        "    # 2. 将标签广播成与特征匹配的形状\n",
        "    # 先将标签扩展到 (5, 5) 的二维张量，再扩展到 (5, 5, 1)\n",
        "    # feature[..., 0]或者feature[:, :, 0]： 提取了第一个特征通道的二维切片，其形状为 (5, 5)。\n",
        "    # tf.ones_like(feature[:, :, 0]) 生成了一个与这个切片形状相同的张量，值全为 1，形状为 (5, 5)。\n",
        "    # label * tf.ones_like(feature[:, :, 0]) 将标量 label（形状为 ()）扩展为一个形状为 (5, 5) 的二维张量，每个元素的值都等于 label。\n",
        "    # tf.expand_dims(..., axis=-1)在最后一个维度上为标签添加一个新维度。目的：将标签的形状与特征数据的通道维度对齐（即从二维变为三维）。\n",
        "    expanded_label = tf.expand_dims(label * tf.ones_like(feature[..., 0]), axis=-1)  # (5, 5, 1)\n",
        "\n",
        "    # 3. 将特征和标签沿通道维度拼接\n",
        "    # 将特征和标签组合为一个四维张量 combined，使得增强操作能够同步作用在特征和标签上。\n",
        "    # feature 是增强前的特征数据，其形状为 (5, 5, 11)，表示 5×5 空间大小，11 个特征通道。\n",
        "    # tf.stack 将两个张量（特征数据和标签数据）沿新的维度进行堆叠。feature 的形状为 (5, 5, 11)。标签经过上述操作后，形状为 (5, 5, 1)。\n",
        "    # 堆叠后的 combined 张量形状为 (5, 5, 12)，表示 11 个特征通道 + 1 个标签通道。\n",
        "    combined = tf.concat([feature, expanded_label], axis=-1)  # (5, 5, 12)\n",
        "\n",
        "    # 4. 数据增强\n",
        "    # 使用 data_augmentation 对组合的张量进行数据增强，保证特征和标签同步增强。\n",
        "    # augmented 是增强后的张量，其形状为 (5, 5, 12)：第三个维度（最后一个维度）包含 12 个通道，前 11 个是增强后的特征数据，最后 1 个是增强后的标签数据。\n",
        "    augmented = data_augmentation(combined)\n",
        "\n",
        "    # 5. 分离增强后的特征和标签\n",
        "    # augmented_feature: 增强后的特征数据。\n",
        "    # augmented_label: 增强后的标签数据，提取了原始广播的标签值，仍保持不变。\n",
        "    # ... 是省略号，表示选取前面所有维度（这里是第 1 和第 2 维，即 (5, 5) 的空间维度）。\n",
        "    # :-1 表示选择最后一个维度（第 3 维）的前 11 个通道。具体来说：从第 0 通道到第 10 通道（不包括第 11 通道）。\n",
        "    # augmented_feature 的形状为 (5, 5, 11)，即增强后的特征数据。\n",
        "    augmented_feature = augmented[..., :-1]  # 取前 11 个通道 (5, 5, 11)\n",
        "    # ... 表示选取前面所有维度（这里是第 1 和第 2 维，即 (5, 5) 的空间维度）。\n",
        "    # -1 表示选择最后一个通道（第 11 通道），即标签通道。\n",
        "    # 选择标签通道后，形状为 (5, 5)。接下来的操作是从 (5, 5) 中提取一个标量标签：[0, 0] 表示取出标签通道的第 (0, 0) 位置的值。由于标签在增强过程中被广播为 (5, 5)，所以整个通道中的值都是一样的，选取任意一个值即可。这里选取了 (0, 0) 位置的值。\n",
        "    # augmented_label 是一个标量，表示增强后的标签值，形状为 ()。\n",
        "    augmented_label = augmented[..., -1, 0, 0]  # 提取标签，取第一个值即可（标量）\n",
        "\n",
        "    # 6. 恢复特征原始格式 (C, H, W)\n",
        "    # 将增强后的特征形状转换回原始格式 (11, 5, 5)\n",
        "    augmented_feature = tf.transpose(augmented_feature, perm=[2, 0, 1])  # (5, 5, 11) -> (11, 5, 5)\n",
        "\n",
        "    return augmented_feature, augmented_label\n",
        "\n",
        "# 3. 对所有样本进行数据增强\n",
        "augmented_features = []\n",
        "augmented_labels = []\n",
        "\n",
        "# 遍历每个样本，对每对 feature 和 label 调用 augment_feature_label 函数进行数据增强。\n",
        "for i in range(features.shape[0]):\n",
        "    feature = tf.convert_to_tensor(features[i], dtype=tf.float32)\n",
        "    label = tf.convert_to_tensor(labels[i], dtype=tf.float32)\n",
        "\n",
        "    aug_feature, aug_label = augment_feature_label(feature, label)\n",
        "    augmented_features.append(aug_feature)\n",
        "    augmented_labels.append(aug_label)\n",
        "\n",
        "# 转换为 NumPy 数组\n",
        "# 将增强后的特征列表转换为四维 NumPy 数组 (num_samples, 11, 5, 5)。\n",
        "augmented_features = np.stack(augmented_features)\n",
        "# 将增强后的标签列表转换为一维 NumPy 数组 (num_samples,)。\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "# 5. 打印验证形状\n",
        "print(\"增强后的特征形状:\", augmented_features.shape)  # (num_samples, 11, 5, 5)\n",
        "print(\"增强后的标签形状:\", augmented_labels.shape)  # (num_samples,)\n",
        "\n",
        "features = augmented_features\n",
        "labels = augmented_labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgFj6oYCjXzi",
        "outputId": "1b9e9c5c-b356-4ac6-e91e-9783c6d94156"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "增强后的特征形状: (71416, 11, 5, 5)\n",
            "增强后的标签形状: (71416,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 每个类别的样本数，确保每个标签区间有 800 个样本\n",
        "# sample_size = 800 #every class with labels smaller 36 meters has over 800 values\n",
        "# #features = np.mean(features, axis=(2, 3)) # patch mean of size * size features\n",
        "\n",
        "# # 生成从 3 到 36 步长为 3 的数字列表，即 [3, 6, 9, ..., 36]\n",
        "# num = (list(range(3, 37, 3))) #create list from 3 to 36 step 3\n",
        "# # 假设每个样本是一个 5x5 的图像块（大小为 5x5，90 个通道）\n",
        "# shape = (11, 5, 5)\n",
        "# # 创建一个初始的数组用于存储特征数据，形状为 (90, 5, 5)\n",
        "# data_bal = np.ones(shape) #create array to fill with features\n",
        "# # 扩展维度，使得形状变为 (1, 11, 5, 5)，这样可以进行拼接\n",
        "# data_bal = np.expand_dims(data_bal, axis=0) #expand one dimension to concatenate\n",
        "# # 创建一个用于存储标签的初始数组，形状为 (1,)\n",
        "# data_lab = np.ones(1) #create array to fill labels\n",
        "\n",
        "# # 在抽样之前，打印每个区间的样本数量，确保逻辑合理。\n",
        "# # 遍历每个标签区间\n",
        "# for i in num:\n",
        "#   # 注意这个 i 是区间右端点\n",
        "#   # 从标签中选择属于当前区间的索引\n",
        "#   # np.where() 返回的是一个元组，元组的元素个数取决于判断条件中的数据的维度, 元组的每个元素都是一个 数组，这些数组表示满足条件的元素在原始数组中的索引。因此，需要通过 indices[0] 访问索引数组\n",
        "#   # 如果输入数组是 多维的，返回的元组会包含 每一维的索引数组。例如，若数组是三维的，返回的元组就会包含三个数组，分别表示满足条件的元素在三维空间中每一维的索引。\n",
        "#   indices = np.where((labels > i-3) & (labels <= i)) #select indcies from every 3 meter interval until 36\n",
        "#   print(f\"区间 ({i-3}, {i}] 的样本数: {len(indices[0])}\")\n",
        "\n",
        "#   # 根据样本数决定如何抽样\n",
        "#   if len(indices[0]) < sample_size:\n",
        "#       print(f\"样本不足800，仅有 {len(indices[0])} 个样本，允许重复抽样。\")\n",
        "#       sampled_indices = np.random.choice(indices[0], size=sample_size, replace=True)\n",
        "#   else:\n",
        "#       sampled_indices = np.random.choice(indices[0], size=sample_size, replace=False)\n",
        "\n",
        "\n",
        "#   # 在当前区间中随机抽样 800 个样本\n",
        "#   # sampled_indices = np.random.choice(indices[0].flatten(), size=sample_size, replace=False) #random sample of each interval\n",
        "#   # sampled_indices = np.random.choice(indices[0], size=sample_size, replace=False)\n",
        "\n",
        "#   # 提取对应的特征和标签\n",
        "#   tempx = features[sampled_indices]\n",
        "#   tempy = labels[sampled_indices]\n",
        "#   # 将当前区间的特征和标签拼接到平衡数组中\n",
        "#   data_bal = np.concatenate((data_bal, tempx), axis=0)\n",
        "#   data_lab = np.concatenate((data_lab, tempy), axis=0)\n",
        "\n",
        "# # 处理 labels > 36 的标签，这部分直接拼接\n",
        "# indices = np.where((labels > 36)) #add the values > 36 m, they are so few no sample needed\n",
        "# # sampled_indices = indices[0].flatten()\n",
        "# sampled_indices = indices[0]\n",
        "# tempx = features[sampled_indices]\n",
        "# tempy = labels[sampled_indices]\n",
        "# data_bal = np.concatenate((data_bal[1:], tempx), axis=0)\n",
        "# data_lab = np.concatenate((data_lab[1:], tempy), axis=0)\n",
        "\n",
        "# # # 为了配合可视化界面，这里将data_bal重新赋值给features将data_lab重新赋值给labels，以便后续能够使用统一的变量。\n",
        "# features = data_bal\n",
        "# labels = data_lab\n"
      ],
      "metadata": {
        "id": "-kJxDrQP_Nnb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# random_forest"
      ],
      "metadata": {
        "id": "1H6WWNZK_R8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据集拆分：训练集和测试集\n",
        "# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=3) #create train, test set\n",
        "\n",
        "# features 数组形状为 (num_samples, 11, 5, 5)，意味着每个样本有 11 个特征（或 11 个通道），每个特征是一个 5x5 的空间窗口。\n",
        "features_mean = np.mean(features, axis=(2, 3)) # patch mean of size * size features\n",
        "\n",
        "# 现在 features_mean 的形状是 (num_samples, 11)，适用于NN或者其他传统机器学习算法\n",
        "# 注意: train_test_split 只能处理 NumPy 数组或 Pandas DataFrame，并不能直接处理 TensorFlow Dataset 对象。因此，这部分代码在处理 TensorFlow Dataset 时会出错。\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_mean, labels, test_size = 0.3, random_state=3)\n"
      ],
      "metadata": {
        "id": "Ny0lckRuAQVo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 针对每个特征维度（即 11 个通道）计算统计信息\n",
        "for i in range(features_mean.shape[1]):  # 遍历 11 个特征通道\n",
        "    print(f\"特征通道 {i+1} 的统计信息：\")\n",
        "    print(f\"  最大值: {np.max(features_mean[:, i])}\")\n",
        "    print(f\"  最小值: {np.min(features_mean[:, i])}\")\n",
        "    print(f\"  均值: {np.mean(features_mean[:, i])}\")\n",
        "    print(f\"  标准差: {np.std(features_mean[:, i])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNpQkEPnzed1",
        "outputId": "9a5ec3af-3040-411e-a5a4-7d6105ebb393"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "特征通道 1 的统计信息：\n",
            "  最大值: 1780.4632568359375\n",
            "  最小值: 0.38454437255859375\n",
            "  均值: 263.6430969238281\n",
            "  标准差: 265.048828125\n",
            "特征通道 2 的统计信息：\n",
            "  最大值: 38.94664001464844\n",
            "  最小值: 0.0\n",
            "  均值: 21.826772689819336\n",
            "  标准差: 4.484354019165039\n",
            "特征通道 3 的统计信息：\n",
            "  最大值: 42.849761962890625\n",
            "  最小值: 0.0\n",
            "  均值: 25.163105010986328\n",
            "  标准差: 5.27474308013916\n",
            "特征通道 4 的统计信息：\n",
            "  最大值: 42.6895866394043\n",
            "  最小值: 0.0\n",
            "  均值: 25.163984298706055\n",
            "  标准差: 5.292666435241699\n",
            "特征通道 5 的统计信息：\n",
            "  最大值: 0.9391857385635376\n",
            "  最小值: 0.0\n",
            "  均值: 0.7477108836174011\n",
            "  标准差: 0.13865220546722412\n",
            "特征通道 6 的统计信息：\n",
            "  最大值: 2.5254499912261963\n",
            "  最小值: 0.0\n",
            "  均值: 0.14622250199317932\n",
            "  标准差: 0.09870219230651855\n",
            "特征通道 7 的统计信息：\n",
            "  最大值: 137.1260986328125\n",
            "  最小值: 0.0\n",
            "  均值: 22.970571517944336\n",
            "  标准差: 15.971306800842285\n",
            "特征通道 8 的统计信息：\n",
            "  最大值: 295.717041015625\n",
            "  最小值: 0.0\n",
            "  均值: 54.8883171081543\n",
            "  标准差: 39.68658447265625\n",
            "特征通道 9 的统计信息：\n",
            "  最大值: 457.13922119140625\n",
            "  最小值: 0.0\n",
            "  均值: 83.5848159790039\n",
            "  标准差: 62.21974563598633\n",
            "特征通道 10 的统计信息：\n",
            "  最大值: 0.6766071915626526\n",
            "  最小值: -23.869853973388672\n",
            "  均值: -12.489816665649414\n",
            "  标准差: 2.751420021057129\n",
            "特征通道 11 的统计信息：\n",
            "  最大值: 0.9984122514724731\n",
            "  最小值: -0.998410165309906\n",
            "  均值: -0.011082637123763561\n",
            "  标准差: 0.558259129524231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 特征标准化"
      ],
      "metadata": {
        "id": "MTaCYkbWzpzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在标准化特征时，使用 训练集 来拟合标准化器（如 StandardScaler），并用其参数（均值和标准差）对测试集进行变换，这是一种 防止数据泄露 的必要操作。\n",
        "\n",
        "标准化的正确流程：\n",
        "\n",
        "用训练集计算标准化参数（如均值和标准差）。\n",
        "用这些参数对训练集和测试集分别进行标准化。"
      ],
      "metadata": {
        "id": "yF-PDkPdztqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 初始化标准化器\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit_transform()：计算训练集的统计参数（均值和标准差）。根据这些参数对数据进行标准化。适用于训练集\n",
        "# 仅用训练集数据拟合标准化器（避免数据泄露）\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# transform()：不会重新计算统计参数，而是使用 fit_transform() 计算出的参数对数据进行标准化。适用于测试集（或者未来的预测数据）。\n",
        "# 使用训练集的标准化参数变换测试集\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "wbYtXkq8z0cY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Search**"
      ],
      "metadata": {
        "id": "allfxVTUAKre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 用于保存机器学习模型到指定路径，以便后续可以重新加载并使用，而不需要重新训练模型。\n",
        "def save_model(model, modelname):\n",
        "    \"\"\"\n",
        "    保存机器学习模型。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: sklearn.ensemble.*\n",
        "      训练好的机器学习模型。\n",
        "    model_name: String\n",
        "      模型名称。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the models directory exists\n",
        "    os.makedirs('/content/drive/My Drive/forest_height/models/', exist_ok=True)\n",
        "\n",
        "    # Save the model\n",
        "    # 始终希望启用压缩，可以直接在 joblib.dump 调用中设置 compress。\n",
        "    # joblib.dump 函数支持 compress 参数，用于对保存的模型文件进行压缩。启用压缩不会破坏模型，只是减小文件大小。\n",
        "    # 模型保存时的压缩不会影响模型加载，加载时不需要指定任何参数。\n",
        "    joblib.dump(model, f'/content/drive/My Drive/forest_height/models/{modelname}.joblib', compress=True)\n",
        "\n",
        "    print(f\"Model saved as '/content/drive/My Drive/forest_height/models/{modelname}.joblib'\")\n",
        "\n",
        "    # 加载保存的模型。加载的模型对象与保存前完全一致，可以直接用于推理或评估，无需重新训练。将其单独定义为一个函数更加方便。\n",
        "    # load model with:\n",
        "    # model = joblib.load(\"forest_height/models/{model_name}.joblib\")\n",
        "\n",
        "\n",
        "\n",
        "# load_model 函数返回的模型与save_model(model, modelname)保存的模型完全一致\n",
        "def load_model(modelname):\n",
        "    \"\"\"\n",
        "    Load a previously saved model from a .joblib file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    modelname : str\n",
        "        The name of the saved model file (without extension).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model : object\n",
        "        The loaded machine learning model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the path to the model\n",
        "    model_path = f'/content/drive/My Drive/forest_height/models/{modelname}.joblib'\n",
        "\n",
        "    # Check if the model file exists\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model file '{model_path}' does not exist.\")\n",
        "\n",
        "    # Load and return the model\n",
        "    model = joblib.load(model_path)\n",
        "    print(f\"Model loaded from '{model_path}'\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Nk5_9bCsBBku"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 为了适应不同的数据集，cols 应该动态调整，确保其与数据集的特征对应。\n",
        "def feature_importance(\n",
        "    model,\n",
        "    model_name,\n",
        "    cols = ['Height', 'HH_Dir4_Mean', 'HV_Dir3_Mean', 'HV_Dir4_Mean', 'Entropy', 'mv', 'RLD12', 'RLD20', 'RLD5', 'sigma_db_HV', 'SinAspect']\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Visualize feature importance of regression model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: sklearn.ensemble.*\n",
        "      一个已经训练好的回归模型，必须具有 feature_importances_ 属性。\n",
        "    model_name: String\n",
        "      模型的名称，用于可视化标题中显示。\n",
        "    cols: Array of Strings\n",
        "      list（字符串数组）.特征名称列表，用于匹配模型中的特征顺序。如果为 None，则自动生成 [\"Feature 1\", \"Feature 2\", ...]。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None, just prints out feature importances and plots them in a bar graph\n",
        "      直接打印和绘制特征重要性。\n",
        "    \"\"\"\n",
        "    # 这是一个数组，存储模型中每个特征的重要性分数。\n",
        "    # importance = model.feature_importances_ 这个数组的长度与为喂给model进行训练模型的数据集中的特征数量一致，即importance 的长度等于训练数据集中特征的数量。\n",
        "    importance = model.feature_importances_\n",
        "\n",
        "    # summarize feature importance\n",
        "    # 打印特征重要性\n",
        "    # enumerate(importance) 枚举特征重要性数组，i 是特征索引，v 是对应的重要性分数\n",
        "    for i,v in enumerate(importance):\n",
        "        # 简单地用特征索引（Feature: 0、Feature: 1 等）来表示特征。\n",
        "        print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "\n",
        "    # 如果没有传入 cols，则动态生成\n",
        "    if cols is None:\n",
        "        cols = [f\"Feature {i}\" for i in range(len(importance))]\n",
        "    elif len(cols) != len(importance):\n",
        "        raise ValueError(\"Length of 'cols' does not match number of features in the model.\")\n",
        "\n",
        "    # plot feature importance\n",
        "    # 将图像宽度适当拉长，使其能够容纳更多的特征名称。\n",
        "    # plt.figure(figsize=(20, 10))  # 宽20，高10\n",
        "    # cols：x 轴的特征名称（列表）。  importance：y 轴的特征重要性分数。  color=color：条形图的颜色（需要外部定义 color，否则会报错）。\n",
        "    plt.bar(cols, importance, color=\"#01748F\")\n",
        "    # 设置 x 轴标签\n",
        "    plt.xlabel(\"Features\")\n",
        "    # 设置 y 轴标签\n",
        "    plt.ylabel(\"Feature Importance\")\n",
        "    # 图表标题，显示模型名称\n",
        "    plt.title(f\"Feature Importance of {model_name} Regression\")\n",
        "    plt.xticks(rotation=45)  # 调整标签角度，避免标签之间的重叠\n",
        "    # 显示图表\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Pp8DONKvH3Q5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pred_vs_true(model, model_name):\n",
        "    \"\"\"\n",
        "    Visualize predictions and compare them to the labeled data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: sklearn.ensemble.*\n",
        "      训练好的机器学习模型，用于预测。通过 model.predict(X_test) 生成预测值。\n",
        "    model_name: String\n",
        "      字符串，表示模型名称，用于可视化时的标题显示。\n",
        "\n",
        "    该函数不输入数据集，直接使用在前面代码中划分的数据集数据集即可。\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None, just prints out errors of each dataset\n",
        "      该函数没有返回值，仅通过两种可视化方式展示预测值和真实值的关系：\n",
        "      1.整体预测值 vs. 真实值的散点图。点为蓝色点。展示模型整体性能：预测值和真实值是否接近对角线。\n",
        "      2.单一通道（特征） vs. 森林高度的散点图。黑色点为真实值，蓝色点为预测值。分别展示两个特定通道（第四通道和第五通道）特征与森林高度（真实值和预测值）的关系。帮助分析模型是否在这些特定特征通道上表现良好。\n",
        "    \"\"\"\n",
        "    # Get model predictions\n",
        "    # 注意这是训练好保存下来的机器学习模型，用于预测\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # visualize predictions vs. true labels\n",
        "    # 可视化 1 - 整体预测值 vs. 真实值\n",
        "    fig = plt.figure(figsize=(6,6))\n",
        "    # 绘制 y_pred（预测值）和 y_test（真实值）的散点图。\n",
        "    # color=color 控制点的颜色（需要外部定义），alpha=0.5 设置点的透明度。\n",
        "    plt.scatter(y_pred, y_test, color=\"#01748F\", alpha=0.5)\n",
        "    plt.xticks(rotation=45)\n",
        "    # 坐标轴格式化\n",
        "    # 对 x 轴刻度值进行格式化为无小数点的整数。\n",
        "    plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
        "    # 添加对角线\n",
        "    # 绘制对角线（黑色虚线），表示理想状态下预测值等于真实值（y_pred = y_test）。\n",
        "    plt.plot([-1,75], [-1, 75], 'k--')\n",
        "    # 坐标轴设置\n",
        "    plt.xlabel(\"Predictions\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    # 设置 x 和 y 轴的范围（硬编码）\n",
        "    plt.xlim([-1,75])\n",
        "    plt.ylim([-1,75])\n",
        "    # 根据模型名称动态生成标题\n",
        "    plt.title(f\"{model_name} Regression: Prediction vs. Labels\")\n",
        "    # 显示图像\n",
        "    plt.show()\n",
        "\n",
        "    # 可视化 2 - 单一特征 vs. 森林高度\n",
        "    # 实现绘制两幅图：\n",
        "    # 一幅是HH极化通道的sigmadB（绘制在X轴上）与真实值 y_test 的散点图；\n",
        "    # 另一幅是HV极化通道的sigmadB（绘制在X轴上）与真实值 y_test 的散点图\n",
        "    # 因为太高维的数据对人类来说是无法可视化的\n",
        "\n",
        "    # Part 2: Visualize ninth channel vs. forest height\n",
        "    # 绘制 X_test 的第 9 通道值（HV极化通道的sigmadB）与真实值 y_test 的散点图，颜色为黑色，点大小为 10。\n",
        "    fig, ax = plt.subplots()\n",
        "    # 选择特定特征通道，绘制散点图。\n",
        "    # 提取 X_test 中的第四通道（索引从 0 开始，第 4 通道为索引 3）\n",
        "    plt.scatter(X_test[:,9], y_test, 10, color='black')  # Fourth channel vs. true labels\n",
        "    plt.scatter(X_test[:,9], y_pred, 10, color=\"#01748F\")  # Fourth channel vs. predictions\n",
        "    # 标题\n",
        "    plt.title(f'{model_name} Regression: Sigma0_db_HH and Forest Height')\n",
        "    # 轴\n",
        "    plt.xlabel('Sigma0_db_HH')\n",
        "    plt.ylabel('Forest Height')\n",
        "    # 图例\n",
        "    # 添加图例，标识黑色点为真实值，其他颜色点为预测值。\n",
        "    ax.legend((\"True Value\", \"Prediction\"), loc='upper left')\n",
        "    # 显示图像\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "cuPz5LeHLp5G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, test_features, test_labels):\n",
        "    \"\"\"\n",
        "    Evaluate a model on specified datasets and return comprehensive evaluation metrics.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: sklearn.ensemble.*\n",
        "      A trained model instance (e.g., RandomForestRegressor).\n",
        "    test_features: numpy.ndarray\n",
        "      Test features (X_test), usually a 2D array or matrix.\n",
        "    test_labels: numpy.ndarray\n",
        "      Test labels (y_test), usually a 1D array or vector representing true values.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      A dictionary containing evaluation metrics:\n",
        "      - errors: Absolute errors for each sample (numpy.ndarray)\n",
        "      - mae: Mean Absolute Error (float)\n",
        "      - mse: Mean Squared Error (float)\n",
        "      - rmse: Root Mean Squared Error (float)\n",
        "      - mape: Mean Absolute Percentage Error (float)\n",
        "      - r2: Coefficient of determination (R²) (float)\n",
        "      - accuracy: Model accuracy in percentage (float)\n",
        "    \"\"\"\n",
        "    # 模型预测\n",
        "    predictions = model.predict(test_features)\n",
        "\n",
        "    # 绝对误差\n",
        "    errors = abs(predictions - test_labels)\n",
        "\n",
        "    # 计算 MAE, MSE, RMSE, 和 MAPE\n",
        "    mae = mean_absolute_error(test_labels, predictions)\n",
        "    mse = mean_squared_error(test_labels, predictions)\n",
        "    rmse = mse ** 0.5\n",
        "    mape = mean_absolute_percentage_error(test_labels, predictions) * 100\n",
        "\n",
        "    # 计算 R²\n",
        "    r2 = r2_score(test_labels, predictions)\n",
        "\n",
        "    # 计算准确率 (Accuracy = 100 - MAPE)\n",
        "    accuracy = 100 - mape\n",
        "\n",
        "    # 打印模型评估结果\n",
        "    # 通过访问返回的字典的键，可以获取每个指标的值或者将字典中的值分别赋值给单独的变量。比如：假设 model, X_test, y_test 是已经定义的\n",
        "    # results = evaluate_model_performance(model, X_test, y_test)\n",
        "    # print(\"Errors:\", results['errors'])\n",
        "    # errors = results['errors']\n",
        "    print('Model Performance:')\n",
        "    print('Average Error (Absolute): {:0.4f}'.format(np.mean(errors)))\n",
        "    print('MAE: {:0.4f}'.format(mae))\n",
        "    print('MSE: {:0.4f}'.format(mse))\n",
        "    print('RMSE: {:0.4f}'.format(rmse))\n",
        "    print('MAPE: {:0.2f}%'.format(mape))\n",
        "    print('R²: {:0.4f}'.format(r2))\n",
        "    print('Accuracy: {:0.2f}%'.format(accuracy))\n",
        "\n",
        "    # 返回包含所有指标的字典\n",
        "    return {\n",
        "        'errors': errors,\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'mape': mape,\n",
        "        'r2': r2,\n",
        "        'accuracy': accuracy\n",
        "    }\n"
      ],
      "metadata": {
        "id": "5B5cA05fNenU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def normalize_color_np(img):\n",
        "    \"\"\"\n",
        "    Normalize a multi-band image for visualization (RGB channels).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img : numpy.ndarray\n",
        "        Multi-band image with shape (color_channels, height, width).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray\n",
        "        RGB image with shape (height, width, 3), normalized to [0, 1].\n",
        "    \"\"\"\n",
        "    # 检查输入是否为 3D 图像\n",
        "    assert len(img.shape) == 3, \"Input X must have 3 dimensions (color_channels, height, width).\"\n",
        "\n",
        "    # 提取红色、绿色和蓝色通道（可以根据需求选择不同的通道）\n",
        "    # 提取的 red, green, 和 blue 通道：形状均为 (height, width)\n",
        "    red = img[0, :, :]  # 第 13 通道\n",
        "    green = img[1, :, :]  # 第 14 通道\n",
        "    blue = img[2, :, :]  # 第 15 通道\n",
        "\n",
        "    # 对各通道进行归一化到 [0, 1]\n",
        "    # 归一化后的 red_norm, green_norm, 和 blue_norm：归一化不会改变数组的形状，仍然是 (height, width)。\n",
        "    red_norm = (red - red.min()) / (red.max() - red.min())\n",
        "    green_norm = (green - green.min()) / (green.max() - green.min())\n",
        "    blue_norm = (blue - blue.min()) / (blue.max() - blue.min())\n",
        "\n",
        "    # 合并为 RGB 图像\n",
        "    # axis=-1 表示将输入数组沿新轴堆叠到最后一个维度。堆叠结果将生成一个新的 3D 数组，形状为 (height, width, 3)，即每个像素点对应一个 RGB 值。\n",
        "    return np.stack((red_norm, green_norm, blue_norm), axis=-1)\n",
        "\n",
        "\n",
        "def plot_img_np(img):\n",
        "    \"\"\"\n",
        "    Visualize a single image (either multi-band X or single-band prediction).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img : numpy.ndarray\n",
        "        Input image. Can be:\n",
        "        - A multi-band satellite image (shape: (color_channels, height, width)).\n",
        "        - A single-band prediction (shape: (height, width)).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # 判断输入是否为 3D 图像\n",
        "    if len(img.shape) == 3:  # 多波段图像\n",
        "        img = normalize_color_np(img)  # 调用 normalize_color 进行归一化并转换为 RGB 图像\n",
        "\n",
        "    # 绘制图像\n",
        "    plt.figure(figsize=(6, 6))  # 设置画布大小\n",
        "    plt.imshow(img, cmap='viridis' if len(img.shape) == 2 else None)  # 单波段使用色彩映射，RGB 图像直接显示\n",
        "    plt.colorbar() if len(img.shape) == 2 else None  # 单波段图像显示 colorbar\n",
        "    plt.axis(\"off\")  # 关闭坐标轴\n",
        "    plt.show() # 显示窗口"
      ],
      "metadata": {
        "id": "ieSjXSvBSTCL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "定义参数搜索空间"
      ],
      "metadata": {
        "id": "GisWlwsqaoJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [100, 200, 500] #[int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "# max_features = ['auto', 'sqrt', 'log2']\n",
        "# 'auto' 是 scikit-learn 早期版本中支持的值，但在新版本中被移除了。\n",
        "max_features = ['sqrt', 'log2', None]  # 移除 'auto'\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10, 15]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "              #  squared_error (默认),计算每个分裂节点的均方误差 (Mean Squared Error, MSE)。\n",
        "              #  absolute_error,计算每个分裂节点的平均绝对误差 (Mean Absolute Error, MAE)。\n",
        "              #  'criterion': ['squared_error', 'absolute_error'],  # 确保只有有效值\n",
        "              #  'criterion': ['absolute_error'],\n",
        "               'bootstrap': bootstrap}\n"
      ],
      "metadata": {
        "id": "QYzkoimR_QcY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 检查 X_train 和 y_train 是否有 NaN 或 Inf\n",
        "assert not np.any(np.isnan(X_train)), \"X_train contains NaN values\"\n",
        "assert not np.any(np.isinf(X_train)), \"X_train contains Inf values\"\n",
        "assert not np.any(np.isnan(y_train)), \"y_train contains NaN values\"\n",
        "assert not np.any(np.isinf(y_train)), \"y_train contains Inf values\"\n"
      ],
      "metadata": {
        "id": "W7ueTzq99utT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# %%time 是 单元级别魔法命令，需要放在代码单元的第一行。如果不是第一行，会导致错误。不要在 %%time 前添加任何内容（如注释或空行）。\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# 初始化模型\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# 设置随机搜索\n",
        "rf_random = RandomizedSearchCV(\n",
        "    estimator = rf,\n",
        "    param_distributions = random_grid,\n",
        "    # scoring=\"neg_mean_absolute_error\", # strategy to evaluate the performance\n",
        "    # n_iter = 150,\n",
        "    # n_iter = 80,\n",
        "    n_iter = 50,\n",
        "    cv = 3, # k-fold cross-validation\n",
        "    verbose=2, # the higher, the more messages\n",
        "    random_state=3,\n",
        "    # n_jobs=-1, # use all processors\n",
        "    return_train_score=True,\n",
        "    # 在 RandomizedSearchCV 中设置 error_score='raise'，以便明确了解失败原因：\n",
        "    error_score='raise'  # 遇到错误时抛出异常\n",
        ")\n",
        "\n",
        "# 训练模型\n",
        "rf_random.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "Uv6J-LtHRNyZ",
        "outputId": "9fad4b41-4d4c-4a2c-dafc-5025c1997500",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印最佳参数\n",
        "print(\"Best Parameters:\", rf_random.best_params_)"
      ],
      "metadata": {
        "id": "_Mqm7XhoHpf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 将会打印出最佳超参数\n",
        "# rf_random.best_params_\n",
        "\n",
        "# 保存模型\n",
        "import joblib\n",
        "save_model(rf_random, \"random_forest\")\n",
        "\n",
        "# 保存模型和加载模型时，模型名称都无后缀\n",
        "rf = load_model(\"random_forest\")\n",
        "\n",
        "# 打印特征及其重要性,以及特征重要性可视化\n",
        "# feature_importance(rf, \"random_forest\")\n",
        "# RandomizedSearchCV 本身没有 feature_importances_，但 rf.best_estimator_ 才是真正训练好的 RandomForestRegressor，它有 feature_importances_。\n",
        "feature_importance(rf.best_estimator_, \"random_forest\")\n",
        "\n",
        "\n",
        "# 输出两类图,\n",
        "# 一为 整体预测值 vs. 真实值的散点图,点为蓝色点。\n",
        "# 二为 单一特征 vs. 森林高度。黑色点为真实值，蓝色点为预测值。分别展示两个特定通道（第四通道和第五通道）特征与森林高度（真实值和预测值）的关系。\n",
        "pred_vs_true(rf, \"random_forest\")\n",
        "\n",
        "# 直接将函数返回的字典赋值给一个变量。\n",
        "results = evaluate_model(rf, X_test, y_test)\n",
        "# 通过访问字典的键，可以获取每个指标的值。\n",
        "errors = results['errors']\n",
        "mae = results['mae']\n",
        "mse = results['mse']\n",
        "rmse = results['rmse']\n",
        "mape = results['mape']\n",
        "r2 = results['r2']\n",
        "accuracy = results['accuracy']"
      ],
      "metadata": {
        "id": "A_xgDbbiROgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 加载训练好的模型并预测"
      ],
      "metadata": {
        "id": "igM8kWSSUSER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载保存由测试图像切割而来的图像块的文件夹路径\n",
        "folder_path = '/content/drive/My Drive/data/images/'  #folder path\n",
        "# 加载已训练的 神经网络 / rf 模型\n",
        "rfmodel = load_model(\"random_forest\")\n",
        "# ind 是保存由测试图像切割得的图像块文件时的索引，以确保文件名唯一。\n",
        "ind = 0\n",
        "\n",
        "# Iterate over the files in the folder\n",
        "# 遍历文件夹中的图像块\n",
        "for filename in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "      # 加载图像块\n",
        "      # X 的形状应该是 (11, 1024, 1024)。\n",
        "      X = np.load(file_path) #load patch\n",
        "\n",
        "      # 网络的输入应该是 (batch_size, feature_size)，因此应该将 X 直接展平为 (batch_size, 11)。这样才符合神经网络的输入要求（即每个样本有 11 个特征）。\n",
        "      # 将图像块重塑为 (11, -1) 形状，并转置. 即将 X 重塑为了(11, 1024 * 1024), 转置以后形状为(1024 * 1024, 11)\n",
        "      # 此时每一行代表一个像素点的所有特征（即每个像素的 11 个波段的值），而列数代表特征的维度。这样做的原因是因为神经网络的输入需要每个样本是一个向量，每个样本代表一个像素点的 11 个特征。这样处理后的数据可以直接传入神经网络。\n",
        "      Xr = X.reshape(11,-1).transpose() #transpose patch\n",
        "\n",
        "      # 特征标准化\n",
        "      # from sklearn.preprocessing import StandardScaler\n",
        "      # 初始化标准化器\n",
        "      scaler = StandardScaler()\n",
        "      # fit_transform()：计算训练集的统计参数（均值和标准差）。根据这些参数对数据进行标准化。适用于训练集\n",
        "      # 仅用训练集数据拟合标准化器（避免数据泄露）\n",
        "      Xr = scaler.fit_transform(Xr)\n",
        "\n",
        "\n",
        "      # 使用 神经网络 / rf 进行预测\n",
        "      # 神经网络预测时需要一个二维矩阵，其中每行是一个样本（像素点）的特征，形状是 (batch_size, 11)，其中 batch_size 是像素点的数量（即 size * size），而 11 是特征的维度。\n",
        "      # rfpred 的形状应该是 (size * size, 1)的二维数组，表示每个像素的预测值, 而不是 (size * size,)的一维数组。这一点也可以从后一句\"将预测结果重塑为图像块大小\"的代码看出.\n",
        "      rfpred = rfmodel.predict(Xr) #predict labels\n",
        "      # 将预测结果重塑为图像块大小\n",
        "      # transpose() 会将 nnpred 从 (size * size, 1) 转换为 (1, size * size)。\n",
        "      # .reshape(1, 1024, 1024) 会将其重塑为一个大小为 (1, 1024, 1024) 的三维数组，这表示你正在把预测结果重新调整为图像块的形状。\n",
        "      rfpredr = rfpred.transpose().reshape(1,1024,1024) #reshape patch to image size\n",
        "      # 保存预测结果\n",
        "      np.save('/content/drive/My Drive/forest_height/MaskRF/mask_'+ str(ind) + '.npy', rfpredr) #save patch\n",
        "      # 更新索引\n",
        "      ind = ind + 1\n",
        "      # 输出预测结果的分位数\n",
        "      print(np.percentile(rfpred[:], [1, 25, 50, 75, 99]))\n",
        "      # 输出最小的 10 个预测值\n",
        "      print(np.sort(rfpred.flatten())[:10])\n",
        "      # 输出最大的 10 个预测值\n",
        "      print(np.sort(rfpred.flatten())[-10:][::-1])"
      ],
      "metadata": {
        "id": "dqUsO4xbUReY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 重建预测图像并保存："
      ],
      "metadata": {
        "id": "h7BPexC9Ur5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 预测结果文件夹路径\n",
        "folder_path = '/content/drive/My Drive/forest_height/MaskRF/'  #folder path\n",
        "img_list = []\n",
        "\n",
        "# Iterate over the files in the folder\n",
        "# 遍历文件夹中的预测图像块\n",
        "for filename in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        # Load the data from the file\n",
        "        data = np.load(file_path) #加载预测结果\n",
        "        # Append the data to the list\n",
        "        img_list.append(data)  #将预测结果添加到列表中\n",
        "\n",
        "# 我有 143 个切片（按 13 x 11 进行排列），可以利用这些切片的索引按列和按行拼接来恢复完整的图像。\n",
        "# 假设img_list包含所有的切片，已经按正确顺序加载\n",
        "m = 11  # 高度方向的切片数量\n",
        "n = 13  # 宽度方向的切片数量\n",
        "\n",
        "# 将每13个图像块按列(宽度方向)拼接\n",
        "# Concatenate the patches along the columns (horizontal axis)\n",
        "# 按列拼接每行的切片\n",
        "rows = []\n",
        "for i in range(m):\n",
        "    row = np.concatenate(img_list[i*n:(i+1)*n], axis=2)  # 按列拼接\n",
        "    rows.append(row)\n",
        "\n",
        "# 按行拼接\n",
        "original_image = np.concatenate(rows, axis=1)  # 按行拼接\n",
        "\n",
        "# im1 = np.concatenate((img_list[0], img_list[1], img_list[2], img_list[3]), axis=2)\n",
        "# im2 = np.concatenate((img_list[4], img_list[5], img_list[6], img_list[7]), axis=2)\n",
        "# im3= np.concatenate((img_list[8], img_list[9], img_list[10], img_list[11]), axis=2)\n",
        "# im4 = np.concatenate((img_list[12], img_list[13], img_list[14], img_list[15]), axis=2)\n",
        "\n",
        "# # 再将四个拼接好的部分按行(高度方向)拼接\n",
        "# # Concatenate the rows along the vertical axis to rebuild the original image\n",
        "# original_image = np.concatenate((im1, im2, im3, im4), axis=1)\n",
        "\n",
        "#保存重建后的完整预测图像\n",
        "np.save('/content/drive/My Drive/forest_height/MaskRF/FinalPredictions/mask_private_rf.npy', original_image)\n"
      ],
      "metadata": {
        "id": "4Kl2HJtvUY-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 可视化预测结果"
      ],
      "metadata": {
        "id": "jDCSuRw4cN6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rebuild_image = np.load('/content/drive/My Drive/forest_height/MaskRF/FinalPredictions/mask_private_rf.npy')\n",
        "# 将重建的预测图像由 (1, height, width) 转换为 (height, width)\n",
        "tree_height_2d = plot_rebuild_image[0]\n",
        "\n",
        "# 使用 `matplotlib` 绘制图像\n",
        "# Plot the tree height data\n",
        "plt.imshow(tree_height_2d, cmap='viridis')  # 使用 `viridis` 色图\n",
        "\n",
        "# Add colorbar for reference\n",
        "plt.colorbar()  # 添加颜色条\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QsnijbSIcNWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 输出预测结果的分位数和极值"
      ],
      "metadata": {
        "id": "cwaqQpl5UZxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.percentile(plot_rebuild_image[:], [1, 25, 50, 75, 99])) #calculate quantiles 0.01, 0.25, 0.5, 0.75, 0.99\n",
        "\n",
        "print(np.sort(plot_rebuild_image.flatten())[:10]) #print the 10 lowest predictions\n",
        "print(np.sort(plot_rebuild_image.flatten())[-10:][::-1]) #print the 10 highest predictions"
      ],
      "metadata": {
        "id": "GsxUbvqkxMS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 预测\n",
        "# img = rf.predict(X_test)\n",
        "\n",
        "# # 检查img的形状\n",
        "# print(f\"Shape of img: {img.shape}\")\n",
        "\n",
        "# # plot_img_np(img)\n",
        "\n",
        "# # 获取原始掩膜的尺寸\n",
        "# # H, W = original_mask.shape  # 原始影像尺寸\n",
        "# H, W = 11155, 13083  # 原始影像尺寸\n",
        "\n",
        "# # 检查形状是否一致\n",
        "# print(f\"indices_all shape: {indices_all.shape}\")\n",
        "# print(f\"img shape: {img.shape}\")\n",
        "\n",
        "# # 修正索引数量\n",
        "# if indices_all.shape[0] > img.shape[0]:\n",
        "#     print(\"Warning: indices_all has more entries than predictions. Adjusting...\")\n",
        "#     indices_all = indices_all[:img.shape[0]]\n",
        "\n",
        "# # 创建空矩阵用于存储预测值\n",
        "# prediction_map = np.full((H, W), np.nan)  # 设为 NaN，未预测的位置\n",
        "\n",
        "# # 用修正后的 `indices_all` 映射预测结果\n",
        "# prediction_map[indices_all[:, 0].astype(int), indices_all[:, 1].astype(int)] = img\n",
        "\n",
        "# # 可视化结果\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# plt.imshow(prediction_map, cmap='viridis', interpolation='nearest')\n",
        "# plt.colorbar(label=\"Predicted Forest Height\")\n",
        "# plt.title(\"Predicted Forest Height Map\")\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4WG5wtXwBLK4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
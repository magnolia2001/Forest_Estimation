{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiJoGfEgNp5cIRzQFWYEkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magnolia2001/Forest_Estimation/blob/main/notebooks/NNandCNN_FeatureNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 在划分训练集和测试集之前对特征进行标准化\n",
        "\n",
        "且不进行数据平衡"
      ],
      "metadata": {
        "id": "FjqrJSYP6MZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrFMkyfpaHR0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 检查挂载的路径结构\n",
        "!ls /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = '/content/drive/My Drive/data/'\n",
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'"
      ],
      "metadata": {
        "id": "JRDi_LKk5Q9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime, os, cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
        "# from keras.models import Sequential, load_model\n",
        "# from keras.layers import Dense, BatchNormalization, Dropout, InputLayer, Flatten, Conv2D, MaxPool2D, AveragePooling2D\n",
        "# from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, InputLayer, Flatten, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "\n",
        "\n",
        "# 制作标签数据和特征数据\n",
        "\n",
        "# 窗口大小应为奇数，以保证标签在中间\n",
        "size = 5 #define window size should be odd so that the label is in the middle\n",
        "# 特征的形状，这里假设每个特征是一个大小为 (size, size) 的窗口，包含 11 个通道\n",
        "shape = (11, size, size) #define shape of features\n",
        "# np.ones(shape, dtype=None) 用于创建一个形状为 shape 的数组，并将所有元素初始化为 1.0\n",
        "# 其中 shape：指定数组的形状，通常是一个整数或元组。 dtype：指定数组元素的数据类型（可选）。如果不指定，默认使用 float64 类型。\n",
        "# labels1 用于存放标签数据（掩膜）, data1 用于存放提取的特征数据\n",
        "# np.ones(1)返回的是一个只有一个元素的数组，其中该元素值为 1。\n",
        "labels1 = np.ones(1) #array for labels\n",
        "# 创建了一个数组，形状为 shape 即 (11, 5, 5) 的 NumPy 数组，并且所有的元素值都被初始化为 1.0 。\n",
        "data1 = np.ones(shape) #array for features\n",
        "# 扩展维度，便于后续拼接操作\n",
        "data1 = np.expand_dims(data1, axis=0) #expand dimension to concatenate\n",
        "\n",
        "# 遍历目录中的图像（假设有 20 张图像, 具体数量还需要根据自己的情况修改）\n",
        "# 在 for j in range(20) 这个遍历过程中，区分 j < 10 和 j >= 10 的目的是为了处理不同的文件命名规则。\n",
        "# 对于小于 10 的文件名，文件名是 \"image_00X.npy\"，其中 X 是单个数字（0 到 9）。\n",
        "# 对于大于等于 10 的文件名，文件名是 \"image_0XY.npy\"，其中 XY 是两位数的数字（10 到 19）。\n",
        "for j in range(142): #iterate over images in directory\n",
        "  if j < 10:\n",
        "    # 路径填写实际路径\n",
        "    # 读取图像数据\n",
        "    X = np.load(f'{path_images}image_00'+ str(j) + '.npy')\n",
        "    # 读取掩膜数据\n",
        "    y = np.load(f'{path_masks}mask_00'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "\n",
        "    # 选择掩膜图像 y 中所有大于 0 的位置（即标签不为 0 的位置），并返回这些位置的索引。\n",
        "    # indices 数组返回 N 个元素，其中 N 为掩膜图像 y 中所有大于 0 的元素个数。每个元素都是一个长度为 2 的行向量，表示符合条件元素的行列索引。\n",
        "    # y > 0 是一个布尔条件，返回一个与 y 相同形状的布尔数组, 如果是大于 0，布尔值为 True，否则为 False。\n",
        "    # np.argwhere() 是 NumPy 库中的一个函数，它返回数组中满足某个条件的所有索引（行列坐标），即满足条件的元素的坐标位置。\n",
        "    # 这里 indices 是一个形状为 (N, 2) 的二维数组，每一行是 (y, x) 坐标. y 为行索引, x 为列索引\n",
        "    indices = np.argwhere(y > 0) #select all values with label\n",
        "\n",
        "    # indices_2d 是 indices 数组的一个切片，是一个 二维数组, 它包含了所有掩膜图像中标签值大于 0 的位置的 列索引。\n",
        "    # 切片操作 indices[:, 1:] 就是提取所有行中的第二列（即 行 和 列 坐标中的 列索引）。\n",
        "    # indices_2d = indices[:, 1:] #extract indices\n",
        "\n",
        "    # 初始化一个数组 ind_y 来收集符合条件的标签位置。\n",
        "    # np.ones(2) 会创建一个包含 2 个元素的数组，所有元素的值为 1. 。\n",
        "    # .reshape(-1, 2) 将该数组的形状重塑为 (-1, 2)，表示按列数为 2 进行重塑，-1 表示自动计算行数。由于只有 2 个元素，这会将数组变成形状为 (1, 2) 的二维数组。\n",
        "    ind_y = np.ones(2).reshape(-1,2) #array to collect indices\n",
        "\n",
        "    # 遍历掩膜中的每个标签位置\n",
        "    # for i in indices_2d: #iterate over indices\n",
        "    for i in indices:\n",
        "      # 提取图像块，并检查其形状。\n",
        "      # size//2 表示 size 除以 2 的整数部分，用于确定图像块中心点到边界的距离。右端点的值之所以加 1 是因为区间是左闭右开的,所以加 1 保证能取到右端点.\n",
        "      # 整个i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2)表达式计算出一个范围，用于选取以 (i[0], i[1])（标签的 y, x 坐标） 为中心，上下各延伸 size//2 个像素的区域。\n",
        "      # i[0] 是当前标签位置的 y 坐标（行索引）。i[1] 是当前标签位置的 x 坐标（列索引）。\n",
        "      # 利用 shape 确保当前窗口大小与指定的窗口大小一致\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape: #select only features with the same shape because of labels at the image border\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1] #save them temporary\n",
        "        # 将 temp 的维度扩展一个维度，使得它变成一个形状为 (1, channels, size, size) 的四维数组。扩展维度的目的是为了能够将 temp 与其他提取的窗口进行拼接。\n",
        "        temp2 = np.expand_dims(temp, axis=0) #expand dimension to concatenate\n",
        "        # 拼接特征数据\n",
        "        # data1 最终会变成 (num_samples, 11, 5, 5)，其中 num_samples 是提取的窗口数量。\n",
        "        data1 = np.concatenate((data1, temp2), axis=0) #concatenation\n",
        "        # 拼接标签索引\n",
        "        # i.reshape(-1, 2) 会把 i 重新调整为一个形状为 (1, 2) 的二维数组\n",
        "        # axis=0 表示在 第 0 维（行方向） 进行拼接，即新添加的行会被添加到原数组的最后。\n",
        "        # ind_y 则是一个 (num_samples, 2) 的数组，每个样本对应一个标签位置的 (y, x) 坐标。\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0) #concatenation of index so that they have the same order and length as the features\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    # 初始时，ind_y 中的第一个元素是 np.ones(2).reshape(-1,2) 创建的虚拟数据。此步骤是将它移除，只保留实际的标签坐标。\n",
        "    ind_y = ind_y[1:] #remove first dummy values\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # 提取标签值\n",
        "    # 从这句代码应该可以看出原作者的掩膜图像的形状包含了通道维度,即形状为(1, height, width)\n",
        "    # data_y = y[0, indices_1, indices_2] #extract labels\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0) #concatenate labels\n",
        "\n",
        "  if j >= 10 and j < 100:\n",
        "    X = np.load(f'{path_images}image_0'+ str(j) + '.npy')\n",
        "    y = np.load(f'{path_masks}mask_0'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "    indices = np.argwhere(y > 0)\n",
        "    # indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    # for i in indices_2d:\n",
        "    for i in indices:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    ind_y = ind_y[1:]\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # data_y = y[0, indices_1, indices_2]\n",
        "    # 提取标签值\n",
        "    # 每一对 (indices_1[i], indices_2[i]) 会自动匹配，得到对应位置的标签值。\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "  if j >= 100:\n",
        "    X = np.load(f'{path_images}image_'+ str(j) + '.npy')\n",
        "    y = np.load(f'{path_masks}mask_'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "    indices = np.argwhere(y > 0)\n",
        "    # indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    # for i in indices_2d:\n",
        "    for i in indices:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    ind_y = ind_y[1:]\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # data_y = y[0, indices_1, indices_2]\n",
        "    # 提取标签值\n",
        "    # 每一对 (indices_1[i], indices_2[i]) 会自动匹配，得到对应位置的标签值。\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "# 移除第一个虚拟值\n",
        "# data1 的形状会是 (num_samples, 11, 5, 5)，其中 num_samples 是提取的窗口数量（即符合条件的标签数量）。一个四维数组\n",
        "# labels1 的形状会是 (num_samples,)，其中 num_samples 是所有图像中符合条件的标签数量。一个一维数组\n",
        "data1 = data1[1:] #remove first dummy values\n",
        "labels1 = labels1[1:] #remove first dummy values\n",
        "\n",
        "# data1 和 labels1 应该是 一一对应的，因为它们的样本数（num_samples）相同。\n",
        "# 获得标签数据和特征数据\n",
        "features = data1\n",
        "labels = labels1\n"
      ],
      "metadata": {
        "id": "cwuDejh55Va5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "qH6Ck7wc7MuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# features 数组形状为 (num_samples, 11, 5, 5)，意味着每个样本有 11 个特征（或 11 个通道），每个特征是一个 5x5 的空间窗口。\n",
        "features_mean = np.mean(features, axis=(2, 3)) # patch mean of size * size features\n",
        "\n",
        "# 现在 features_mean 的形状是 (num_samples, 11)，适用于NN或者其他传统机器学习算法\n",
        "# 注意: train_test_split 只能处理 NumPy 数组或 Pandas DataFrame，并不能直接处理 TensorFlow Dataset 对象。因此，这部分代码在处理 TensorFlow Dataset 时会出错。\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_mean, labels, test_size = 0.3, random_state=3)\n"
      ],
      "metadata": {
        "id": "0eSNeRy4568n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以根据统计信息来决定是否需要对特征进行标准化处理"
      ],
      "metadata": {
        "id": "pyD_bqRA7lwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印 features_mean 的统计信息\n",
        "print(\"features_mean 的统计信息：\")\n",
        "print(f\"最大值: {np.max(features_mean)}\")\n",
        "print(f\"最小值: {np.min(features_mean)}\")\n",
        "print(f\"均值: {np.mean(features_mean)}\")\n",
        "print(f\"标准差: {np.std(features_mean)}\")\n"
      ],
      "metadata": {
        "id": "ITD_RgPU7WlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 针对每个特征维度（即 11 个通道）计算统计信息\n",
        "for i in range(features_mean.shape[1]):  # 遍历 11 个特征通道\n",
        "    print(f\"特征通道 {i+1} 的统计信息：\")\n",
        "    print(f\"  最大值: {np.max(features_mean[:, i])}\")\n",
        "    print(f\"  最小值: {np.min(features_mean[:, i])}\")\n",
        "    print(f\"  均值: {np.mean(features_mean[:, i])}\")\n",
        "    print(f\"  标准差: {np.std(features_mean[:, i])}\")\n"
      ],
      "metadata": {
        "id": "6wWkZ-M67gCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "在标准化特征时，使用 训练集 来拟合标准化器（如 StandardScaler），并用其参数（均值和标准差）对测试集进行变换，这是一种 防止数据泄露 的必要操作。"
      ],
      "metadata": {
        "id": "lv4JTont8YZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "标准化的正确流程：\n",
        "1. 用训练集计算标准化参数（如均值和标准差）。\n",
        "2. 用这些参数对训练集和测试集分别进行标准化。"
      ],
      "metadata": {
        "id": "OevE-1UY8cQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 初始化标准化器\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit_transform()：计算训练集的统计参数（均值和标准差）。根据这些参数对数据进行标准化。适用于训练集\n",
        "# 仅用训练集数据拟合标准化器（避免数据泄露）\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# transform()：不会重新计算统计参数，而是使用 fit_transform() 计算出的参数对数据进行标准化。适用于测试集（或者未来的预测数据）。\n",
        "# 使用训练集的标准化参数变换测试集\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "CyPjz0gX8XZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "kC-g42jz8hW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 神经网络模型（NN模型）定义"
      ],
      "metadata": {
        "id": "qY26zkSm8nLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 Sequential 模型，适用于线性堆叠的神经网络。\n",
        "modelNn = Sequential()  # build neural network\n",
        "\n",
        "# 第一层：Dense(128, input_shape=(11,), ...)代表输入层，输入形状为 11,)，表示每个输入样本有 11 个特征。这个 11 是硬编码的，但实际上它应该等于特征的维度数，这里可能需要根据实际情况进行修改。\n",
        "# 如果特征数是动态变化的，建议不要硬编码这个数字，而是通过 features.shape[1] 获取动态的特征维度。\n",
        "\n",
        "input_dim = features.shape[1]  # 动态获取特征维度\n",
        "modelNn.add(Dense(128, input_shape=(input_dim,), kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# 第二层和第三层：分别包含 256 个神经元，也是全连接层，使用 ReLU 激活函数。\n",
        "modelNn.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "modelNn.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "modelNn.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# 使用 Dropout(0.4) 防止过拟合，随机丢弃 40% 的神经元\n",
        "modelNn.add(Dropout(0.4))\n",
        "\n",
        "# 输出层包含 1 个神经元，activation='linear' 表示线性激活函数，适用于回归任务。\n",
        "modelNn.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
        "\n",
        "\n",
        "# 打印模型的结构，包括每一层的类型、输出形状、参数数量等信息，帮助你了解模型的结构。\n",
        "modelNn.summary()\n"
      ],
      "metadata": {
        "id": "7TG9BKhk8ju6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 损失函数：loss='mean_absolute_error' 使用绝对误差作为回归问题的损失函数。\n",
        "# 优化器：optimizer='adam' 使用 Adam 优化器，它是目前常用的一种高效的优化算法。\n",
        "# 评估指标：metrics=['mean_absolute_percentage_error'] 使用平均绝对百分比误差（MAPE）作为评估指标。\n",
        "\n",
        "# 注意: 在训练过程中，优化算法会根据 val_loss 来更新模型的参数，因为 val_loss 是损失函数的值，而损失函数通常是模型优化的目标。\n",
        "#    MAPE 作为评估指标，不会直接影响模型的参数更新，它仅用于评价模型在验证集上的相对误差，帮助你了解模型的实际表现。\n",
        "modelNn.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_percentage_error']) #compile model\n"
      ],
      "metadata": {
        "id": "XRSIGjCG86Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成一个包含当前时间戳的日志目录路径。为 TensorBoard 准备日志文件存储位置。\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))  # log directory for tensorboard\n",
        "\n",
        "# 在训练过程中将日志信息保存到指定的 logdir 目录，并设置每个 epoch 保存权重的直方图。在训练过程中启用 TensorBoard 回调，确保训练日志被记录。\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# 模型保存, 仅保存表现最好的模型。\n",
        "# HDF5 格式的模型文件，请确保扩展名为 .h5 或 .hdf5\n",
        "os.makedirs(\"/content/drive/My Drive/NNmodels\", exist_ok=True)\n",
        "\n",
        "model_save = ModelCheckpoint(\n",
        "    \"/content/drive/My Drive/NNmodels/best_NNmodel.keras\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False  # 保存完整模型（包括架构、权重和优化器状态）\n",
        ")\n"
      ],
      "metadata": {
        "id": "e-AS9nOw8_Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 model.fit() 训练模型：\n",
        "modelNn.fit(X_train, y_train, epochs = 100, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, model_save]) #fit model"
      ],
      "metadata": {
        "id": "6GQCRdpx9J1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras 中的函数，用于加载深度学习模型。\n",
        "bmodel = load_model('/content/drive/My Drive/NNmodels/best_NNmodel.keras') #load best model"
      ],
      "metadata": {
        "id": "QlaCS8V99QcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用训练好的模型对测试集 X_test 进行预测，返回预测值 ypred_nn。\n",
        "ypred_nn = bmodel.predict(X_test)\n"
      ],
      "metadata": {
        "id": "GcRg2ddz9U_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算测试集的均方误差。 (MSE)\n",
        "mse_nn = mse(y_test, ypred_nn)\n",
        "# 计算均方根误差 (RMSE)\n",
        "rmse_nn = mse_nn ** (1/2)\n",
        "# 计算平均绝对误差 (MAE)\n",
        "mae_nn = mae(y_test, ypred_nn)\n",
        "# 平均绝对百分比误差 (MAPE)\n",
        "mape_nn = mape(y_test, ypred_nn)\n",
        "\n",
        "# 打印出 MAPE、MAE 和 RMSE 评估指标，帮助你评估模型的表现\n",
        "print(mape_nn)\n",
        "print(mae_nn)\n",
        "print(rmse_nn)\n"
      ],
      "metadata": {
        "id": "IQYvhYgs9VlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "2J0nE48b9XyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算预测结果的均值\n",
        "mean_nn = np.mean(ypred_nn[:])  # calculate mean\n",
        "# 计算预测结果的不同分位数（1%, 25%, 50%, 75%, 99%）\n",
        "quantiles_nn = np.percentile(ypred_nn[:], [1, 25, 50, 75, 99])  # calculate quantiles 0.01, 0.25, 0.5, 0.75, 0.99\n",
        "\n",
        "# 计算标签的均值\n",
        "mean_labels = np.mean(labels[:])\n",
        "# 计算标签的不同分位数（1%, 25%, 50%, 75%, 99%）\n",
        "quantiles_labels = np.percentile(labels[:], [1, 25, 50, 75, 99])\n",
        "\n",
        "print(mean_nn)\n",
        "print(quantiles_nn)\n",
        "# 打印预测值中最小的 10 个和最大的 10 个。\n",
        "print(np.sort(ypred_nn.flatten())[:10])  # print the 10 lowest predictions\n",
        "print(np.sort(ypred_nn.flatten())[-10:][::-1])  # print the 10 highest predictions\n",
        "\n",
        "print(mean_labels)\n",
        "print(quantiles_labels)\n",
        "# 打印标签中最小的 10 个和最大的 10 个。\n",
        "print(np.sort(labels.flatten())[:10])\n",
        "print(np.sort(labels.flatten())[-10:][::-1])\n"
      ],
      "metadata": {
        "id": "aLkhRG8n9Zcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional neural network"
      ],
      "metadata": {
        "id": "D50-StiF9fwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据集拆分：训练集和测试集\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=3) #create train, test set\n"
      ],
      "metadata": {
        "id": "Mp9sfsYr9hAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 初始化标准化器\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit_transform()：计算训练集的统计参数（均值和标准差）。根据这些参数对数据进行标准化。适用于训练集\n",
        "# 仅用训练集数据拟合标准化器（避免数据泄露）\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# transform()：不会重新计算统计参数，而是使用 fit_transform() 计算出的参数对数据进行标准化。适用于测试集（或者未来的预测数据）。\n",
        "# 使用训练集的标准化参数变换测试集\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "PFCyyW659lfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载 TensorBoard 插件，用于可视化训练过程的日志信息，如损失曲线、指标等。\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "PmkFLTtr9k6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 转换 X_train 和 X_test 的格式为 NHWC\n",
        "# y_train 和 y_test 是目标标签，它们通常是数值（对于回归问题）或分类标签（对于分类问题），是一维数组。目标标签与 CNN 的数据格式无关，因此无需调整。\n",
        "X_train = X_train.transpose(0, 2, 3, 1)  # 从 (N, C, H, W) 转换为 (N, H, W, C)\n",
        "X_test = X_test.transpose(0, 2, 3, 1)\n",
        "\n",
        "# 检查转换后的形状\n",
        "print(X_train.shape)  # 应输出 (49991, 5, 5, 11)\n",
        "print(X_test.shape)   # 应输出 (21426, 5, 5, 11)\n"
      ],
      "metadata": {
        "id": "p3grYI7I9xxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras 默认使用 channels_last 格式，这表示图像的输入形状是 (height, width, channels)。如果想使用 channels_first 格式（即 (channels, height, width)），需要进行相应的设置，比如通过 Keras 配置全局设置或者在模型层中明确指定\n",
        "# 这里采用为每个层指定数据格式。只有卷积层和池化层需要明确指定数据格式（channels_first 或 channels_last），而全连接层不需要这样做，展平层会处理掉格式问题。\n",
        "# 卷积操作会根据指定的数据格式进行计算，输出的形状也会遵循这种数据格式。\n",
        "\n",
        "# 使用 Sequential() 创建一个顺序模型（即按顺序堆叠各层）。\n",
        "modelCnn = Sequential() #bulid cnn\n",
        "\n",
        "# 因为 features 数组形状为 (num_samples, 5, 5, 11)\n",
        "# 指定输入数据的形状。这里假设每个输入图像是一个 11 个通道的 5x5 大小的图像块。\n",
        "modelCnn.add(InputLayer(input_shape=(5, 5, 11)))  # NHWC 格式 # 输入形状为 (height, width，channels)\n",
        "\n",
        "# 注意：在卷积神经网络中，卷积核的形状是 (高度, 宽度, 输入通道数, 输出通道数)，这代表每个卷积核的尺寸、它需要处理的输入通道数，以及它生成的输出通道数。\n",
        "# 高度和宽度决定了卷积核的空间大小，常见的卷积核大小有 3x3、5x5、7x7 等。\n",
        "# 输入通道数是卷积核需要处理的输入特征图的深度（即输入图像的通道数）。\n",
        "# 输出通道数是卷积层的过滤器数目，也就是卷积层最终生成的特征图的数量。\n",
        "\n",
        "# 当你定义一个卷积层时，卷积核的数量会决定卷积层输出的通道数，但卷积核的深度（即每个卷积核的输入通道数）是由输入数据的通道数（即输入特征图的深度）来决定的，而不用手动设置。\n",
        "# Conv2D 添加一个卷积层，filters=128 表示该层将有 128 个卷积核（即输出通道数），每个卷积核对应一个输出通道。\n",
        "# kernel_size=(3,3) 表示卷积核的大小是 3x3。trides=1 表示步幅为 1，即卷积核每次移动 1 个像素。padding=\"same\" 表示零填充（zero padding）策略，目的是使得 卷积操作后 输出特征图的尺寸 保持与输入相同（即宽度和高度保持不变）。activation='relu' 使用 ReLU 激活函数。\n",
        "# 第一层卷积核的形状是 (3, 3, 11，128)，该卷积层的输出图像的形状为(128, 5, 5)，其中 128 是输出的通道数，由于设置了padding = \"same\"，故输出图像大小不变。\n",
        "modelCnn.add(Conv2D(filters=128, kernel_size= (3,3), strides=  1 , padding = \"same\", activation='relu'))\n",
        "\n",
        "# 另一个卷积层，filters=256 代表使用 256 个卷积核（即输出通道数），每个卷积核对应一个输出通道。padding=\"valid\" 表示不使用填充，输出大小会减少。\n",
        "# 第二层卷积核的形状是 (3, 3, 128，256)，该卷积层的输出图像的形状为(256, 3, 3)，其中 256 是输出的通道数，由于使用了 valid padding，输出尺寸减少。\n",
        "modelCnn.add(Conv2D(filters=256, kernel_size= (3,3), strides=  1 , padding = \"valid\", activation='relu'))\n",
        "\n",
        "# MaxPool2D 添加一个最大池化层，pool_size=(2,2) 表示 2x2 的池化窗口，池化操作的 步长 默认是 2，减少空间维度（降低特征图的尺寸）。\n",
        "# 用 2x2 的过滤器，以2为步长进行特征值提取，因此该池化操作会将输入特征图的 空间尺寸（宽度和高度） 缩小一半。采用的池化操作是 最大池化。\n",
        "# 输入尺寸为 (256, 3, 3)，经过池化后，输出的尺寸会变为 (256, 1, 1)，因为池化操作会将 3x3 的特征图缩小为 1x1。\n",
        "modelCnn.add(MaxPool2D(pool_size = (2,2)))\n",
        "\n",
        "# Flatten() 层将二维的特征图展平为一维向量，为全连接层做准备。\n",
        "# Flatten() 会将池化层输出的 (256, 1, 1) 转换为 256 的一维向量。这是因为在全连接层之前，必须将输入转换为一维数据。\n",
        "modelCnn.add(Flatten())\n",
        "\n",
        "# 全连接层处理的是一维向量，而不是多维的图像数据。所以全连接层的输入格式不依赖于 channels_first 或 channels_last。\n",
        "# Dense(512) 添加一个全连接层，包含 512 个神经元，activation='relu' 使用 ReLU 激活函数。\n",
        "modelCnn.add(Dense(512, activation='relu'))\n",
        "\n",
        "# 另一个全连接层，包含 128 个神经元。\n",
        "modelCnn.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Dropout(0.4) 是一个正则化方法，随机丢弃 40% 的神经元，防止过拟合。\n",
        "modelCnn.add(Dropout(0.4))\n",
        "\n",
        "# 最后一层是一个包含 1 个神经元的全连接层，使用线性激活函数（适合回归任务）。\n",
        "modelCnn.add(Dense(1, activation='linear'))\n",
        "\n",
        "\n",
        "# summary() 输出模型的概况，显示每一层的参数数量和输出形状。\n",
        "modelCnn.summary()"
      ],
      "metadata": {
        "id": "yqdCLDGt92vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 compile() 方法指定模型的损失函数、优化器和评估指标。\n",
        "# 使用平均绝对误差作为损失函数. 使用 Adam 优化器. 使用平均绝对百分比误差作为评估指标。\n",
        "modelCnn.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_percentage_error']) #compile cnn\n"
      ],
      "metadata": {
        "id": "xadltJsv-GZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置日志文件夹路径。\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "# TensorBoard 回调用于在训练过程中记录日志，供 TensorBoard 使用。\n",
        "# histogram_freq=1 表示每个 epoch 都记录权重的直方图\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# ModelCheckpoint 回调用于在验证集损失最小化时保存最佳模型。\n",
        "# HDF5 格式的模型文件，请确保扩展名为 .h5 或 .hdf5\n",
        "os.makedirs(\"/content/drive/My Drive/CNNmodels\", exist_ok=True)\n",
        "model_save = ModelCheckpoint(\"/content/drive/My Drive/CNNmodels/best_CNNmodel.keras\",\n",
        "                             save_best_only=True,\n",
        "                             save_weights_only=False)  # directory for best model\n",
        "\n",
        "# model_save = ModelCheckpoint(\"/content/drive/My Drive/CNNmodels/best_CNNmodel.hdf5\", save_best_only = True) #save best model\n"
      ],
      "metadata": {
        "id": "yOidqDVC-Kfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 指定训练数据、测试数据、训练轮次（epochs=100）以及回调函数（tensorboard_callback 和 model_save）。\n",
        "modelCnn.fit(X_train, y_train, epochs = 100, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, model_save]) #train cnn\n"
      ],
      "metadata": {
        "id": "73Za4p2_-QfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bmodel = load_model('/content/drive/My Drive/CNNmodels/best_CNNmodel.keras')"
      ],
      "metadata": {
        "id": "fUxYISpe-Q_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_cnn = bmodel.predict(X_test) #predict best model"
      ],
      "metadata": {
        "id": "aRjfquRD-VZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）和平均绝对百分比误差（MAPE）。\n",
        "# 这些指标用于评估模型在测试集上的性能。\n",
        "mse_cnn = mse(y_test, y_pred_cnn) #calculate metrics\n",
        "rmse_cnn = mse_cnn ** (1/2)\n",
        "mae_cnn = mae(y_test, y_pred_cnn)\n",
        "mape_cnn = mape(y_test, y_pred_cnn)\n",
        "\n",
        "print(mape_cnn)\n",
        "print(mae_cnn)\n",
        "print(rmse_cnn)"
      ],
      "metadata": {
        "id": "s6GI3n_6-WFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 启动 TensorBoard 可视化工具，查看训练过程中的详细信息，如损失曲线、准确率曲线、权重分布等。\n",
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "pdRlGEdT-YKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ypred_cnn[:] 和 labels[:] 都是对数组的切片操作，表示返回数组中的所有元素，分别是预测值和真实标签值。\n",
        "# 无论数组的维度是多少，使用 [:] 都是返回数组中的所有元素。这是 NumPy 中的一个常见用法，它用于获取数组的完整内容，不论数组是多维的。(保持其原有的形状。)\n",
        "\n",
        "# 计算预测结果的均值\n",
        "# ypred_cnn 的形状应为 (n, 1)，即一个列向量，每一行代表一个样本的预测结果。如果你打印 ypred_cnn[:]，你将得到模型对所有测试样本的预测结果。\n",
        "mean_cnn = np.mean(y_pred_cnn[:]) #calculate mean\n",
        "# 计算预测结果的分位数\n",
        "quantiles_cnn = np.percentile(y_pred_cnn[:], [1, 25, 50, 75, 99]) #calculate quantiles 0.01, 0.25, 0.5, 0.75, 0.99\n",
        "\n",
        "# 计算标签的均值\n",
        "# labels 是一个形状为 (n,) 的一维数组，存储了对应测试样本的实际标签（真实的森林高度）。\n",
        "# labels[:] 会返回所有的标签值。\n",
        "mean_labels = np.mean(labels[:])\n",
        "# 计算标签的分位数\n",
        "quantiles_labels = np.percentile(labels[:], [1, 25, 50, 75, 99])\n",
        "\n",
        "print(mean_cnn)\n",
        "print(quantiles_cnn)\n",
        "# 打印预测值中最小的 10 个和最大的 10 个。\n",
        "print(np.sort(y_pred_cnn.flatten())[:10]) #print the 10 lowest predictions\n",
        "print(np.sort(y_pred_cnn.flatten())[-10:][::-1]) #print the 10 highest predictions\n",
        "\n",
        "print(mean_labels)\n",
        "print(quantiles_labels)\n",
        "# 打印标签中最小的 10 个和最大的 10 个。\n",
        "print(np.sort(labels.flatten())[:10])\n",
        "print(np.sort(labels.flatten())[-10:][::-1])"
      ],
      "metadata": {
        "id": "8UxLzttK-YW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
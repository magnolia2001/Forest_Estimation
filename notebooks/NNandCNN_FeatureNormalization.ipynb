{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtzvmUQuuHVO+BNESD9DMP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magnolia2001/Forest_Estimation/blob/main/notebooks/NNandCNN_FeatureNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 在划分训练集和测试集之前对特征进行标准化\n"
      ],
      "metadata": {
        "id": "FjqrJSYP6MZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PrFMkyfpaHR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54701eeb-8061-4df5-997e-0dc6c1711d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 检查挂载的路径结构\n",
        "!ls /content/drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = '/content/drive/My Drive/data/'\n",
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'"
      ],
      "metadata": {
        "id": "JRDi_LKk5Q9f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime, os, cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
        "# from keras.models import Sequential, load_model\n",
        "# from keras.layers import Dense, BatchNormalization, Dropout, InputLayer, Flatten, Conv2D, MaxPool2D, AveragePooling2D\n",
        "# from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, InputLayer, Flatten, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "\n",
        "# 制作标签数据和特征数据\n",
        "\n",
        "# 窗口大小应为奇数，以保证标签在中间\n",
        "size = 5 #define window size should be odd so that the label is in the middle\n",
        "# 特征的形状，这里假设每个特征是一个大小为 (size, size) 的窗口，包含 11 个通道\n",
        "shape = (11, size, size) #define shape of features\n",
        "# np.ones(shape, dtype=None) 用于创建一个形状为 shape 的数组，并将所有元素初始化为 1.0\n",
        "# 其中 shape：指定数组的形状，通常是一个整数或元组。 dtype：指定数组元素的数据类型（可选）。如果不指定，默认使用 float64 类型。\n",
        "# labels1 用于存放标签数据（掩膜）, data1 用于存放提取的特征数据\n",
        "# np.ones(1)返回的是一个只有一个元素的数组，其中该元素值为 1。\n",
        "labels1 = np.ones(1) #array for labels\n",
        "# 创建了一个数组，形状为 shape 即 (11, 5, 5) 的 NumPy 数组，并且所有的元素值都被初始化为 1.0 。\n",
        "data1 = np.ones(shape) #array for features\n",
        "# 扩展维度，便于后续拼接操作\n",
        "data1 = np.expand_dims(data1, axis=0) #expand dimension to concatenate\n",
        "\n",
        "# 遍历目录中的图像（假设有 20 张图像, 具体数量还需要根据自己的情况修改）\n",
        "# 在 for j in range(20) 这个遍历过程中，区分 j < 10 和 j >= 10 的目的是为了处理不同的文件命名规则。\n",
        "# 对于小于 10 的文件名，文件名是 \"image_00X.npy\"，其中 X 是单个数字（0 到 9）。\n",
        "# 对于大于等于 10 的文件名，文件名是 \"image_0XY.npy\"，其中 XY 是两位数的数字（10 到 19）。\n",
        "for j in range(142): #iterate over images in directory\n",
        "  if j < 10:\n",
        "    # 路径填写实际路径\n",
        "    # 读取图像数据\n",
        "    X = np.load(f'{path_images}image_00'+ str(j) + '.npy')\n",
        "    # 读取掩膜数据\n",
        "    y = np.load(f'{path_masks}mask_00'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "\n",
        "    # 选择掩膜图像 y 中所有大于 0 的位置（即标签不为 0 的位置），并返回这些位置的索引。\n",
        "    # indices 数组返回 N 个元素，其中 N 为掩膜图像 y 中所有大于 0 的元素个数。每个元素都是一个长度为 2 的行向量，表示符合条件元素的行列索引。\n",
        "    # y > 0 是一个布尔条件，返回一个与 y 相同形状的布尔数组, 如果是大于 0，布尔值为 True，否则为 False。\n",
        "    # np.argwhere() 是 NumPy 库中的一个函数，它返回数组中满足某个条件的所有索引（行列坐标），即满足条件的元素的坐标位置。\n",
        "    # 这里 indices 是一个形状为 (N, 2) 的二维数组，每一行是 (y, x) 坐标. y 为行索引, x 为列索引\n",
        "    indices = np.argwhere(y > 0) #select all values with label\n",
        "\n",
        "    # indices_2d 是 indices 数组的一个切片，是一个 二维数组, 它包含了所有掩膜图像中标签值大于 0 的位置的 列索引。\n",
        "    # 切片操作 indices[:, 1:] 就是提取所有行中的第二列（即 行 和 列 坐标中的 列索引）。\n",
        "    # indices_2d = indices[:, 1:] #extract indices\n",
        "\n",
        "    # 初始化一个数组 ind_y 来收集符合条件的标签位置。\n",
        "    # np.ones(2) 会创建一个包含 2 个元素的数组，所有元素的值为 1. 。\n",
        "    # .reshape(-1, 2) 将该数组的形状重塑为 (-1, 2)，表示按列数为 2 进行重塑，-1 表示自动计算行数。由于只有 2 个元素，这会将数组变成形状为 (1, 2) 的二维数组。\n",
        "    ind_y = np.ones(2).reshape(-1,2) #array to collect indices\n",
        "\n",
        "    # 遍历掩膜中的每个标签位置\n",
        "    # for i in indices_2d: #iterate over indices\n",
        "    for i in indices:\n",
        "      # 提取图像块，并检查其形状。\n",
        "      # size//2 表示 size 除以 2 的整数部分，用于确定图像块中心点到边界的距离。右端点的值之所以加 1 是因为区间是左闭右开的,所以加 1 保证能取到右端点.\n",
        "      # 整个i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2)表达式计算出一个范围，用于选取以 (i[0], i[1])（标签的 y, x 坐标） 为中心，上下各延伸 size//2 个像素的区域。\n",
        "      # i[0] 是当前标签位置的 y 坐标（行索引）。i[1] 是当前标签位置的 x 坐标（列索引）。\n",
        "      # 利用 shape 确保当前窗口大小与指定的窗口大小一致\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape: #select only features with the same shape because of labels at the image border\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1] #save them temporary\n",
        "        # 将 temp 的维度扩展一个维度，使得它变成一个形状为 (1, channels, size, size) 的四维数组。扩展维度的目的是为了能够将 temp 与其他提取的窗口进行拼接。\n",
        "        temp2 = np.expand_dims(temp, axis=0) #expand dimension to concatenate\n",
        "        # 拼接特征数据\n",
        "        # data1 最终会变成 (num_samples, 11, 5, 5)，其中 num_samples 是提取的窗口数量。\n",
        "        data1 = np.concatenate((data1, temp2), axis=0) #concatenation\n",
        "        # 拼接标签索引\n",
        "        # i.reshape(-1, 2) 会把 i 重新调整为一个形状为 (1, 2) 的二维数组\n",
        "        # axis=0 表示在 第 0 维（行方向） 进行拼接，即新添加的行会被添加到原数组的最后。\n",
        "        # ind_y 则是一个 (num_samples, 2) 的数组，每个样本对应一个标签位置的 (y, x) 坐标。\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0) #concatenation of index so that they have the same order and length as the features\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    # 初始时，ind_y 中的第一个元素是 np.ones(2).reshape(-1,2) 创建的虚拟数据。此步骤是将它移除，只保留实际的标签坐标。\n",
        "    ind_y = ind_y[1:] #remove first dummy values\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # 提取标签值\n",
        "    # 从这句代码应该可以看出原作者的掩膜图像的形状包含了通道维度,即形状为(1, height, width)\n",
        "    # data_y = y[0, indices_1, indices_2] #extract labels\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0) #concatenate labels\n",
        "\n",
        "  if j >= 10 and j < 100:\n",
        "    X = np.load(f'{path_images}image_0'+ str(j) + '.npy')\n",
        "    y = np.load(f'{path_masks}mask_0'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "    indices = np.argwhere(y > 0)\n",
        "    # indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    # for i in indices_2d:\n",
        "    for i in indices:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    ind_y = ind_y[1:]\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # data_y = y[0, indices_1, indices_2]\n",
        "    # 提取标签值\n",
        "    # 每一对 (indices_1[i], indices_2[i]) 会自动匹配，得到对应位置的标签值。\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "  if j >= 100:\n",
        "    X = np.load(f'{path_images}image_'+ str(j) + '.npy')\n",
        "    y = np.load(f'{path_masks}mask_'+ str(j) + '.npy')\n",
        "    # 移除掩膜图像中的通道维度使其形状变为(height, width)\n",
        "    # y = y[0, :, :]  # 去掉通道维度，保留二维掩膜图像\n",
        "    indices = np.argwhere(y > 0)\n",
        "    # indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    # for i in indices_2d:\n",
        "    for i in indices:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    # 去掉第一个虚拟值\n",
        "    ind_y = ind_y[1:]\n",
        "    # 提取所有的行索引\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    # 提取所有的列索引\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    # data_y = y[0, indices_1, indices_2]\n",
        "    # 提取标签值\n",
        "    # 每一对 (indices_1[i], indices_2[i]) 会自动匹配，得到对应位置的标签值。\n",
        "    data_y = y[indices_1, indices_2]  # 提取标签\n",
        "    # 拼接标签，形成最终的标签数组。\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "# 移除第一个虚拟值\n",
        "# data1 的形状会是 (num_samples, 11, 5, 5)，其中 num_samples 是提取的窗口数量（即符合条件的标签数量）。一个四维数组\n",
        "# labels1 的形状会是 (num_samples,)，其中 num_samples 是所有图像中符合条件的标签数量。一个一维数组\n",
        "data1 = data1[1:] #remove first dummy values\n",
        "labels1 = labels1[1:] #remove first dummy values\n",
        "\n",
        "# data1 和 labels1 应该是 一一对应的，因为它们的样本数（num_samples）相同。\n",
        "# 获得标签数据和特征数据\n",
        "features = data1\n",
        "labels = labels1\n"
      ],
      "metadata": {
        "id": "cwuDejh55Va5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 检查 TensorFlow 版本\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebvXEcjYzVTB",
        "outputId": "956fcac6-f1ea-4a7b-aaab-1b45c11b68d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据增强"
      ],
      "metadata": {
        "id": "oxuJ5nXIrpTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# 1. 数据增强管道\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    # TensorFlow 2.5 及以上\n",
        "    # RandomRotation: 随机旋转特征数据，最大旋转角度为 ±40% 的全角。\n",
        "    # 这些增强操作仅应用于特征数据，标签数据保持不变。\n",
        "\n",
        "    # RandomFlip: 随机水平和垂直翻转特征数据。\n",
        "    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    # 在 Python 语法中，列表中最后一个元素后面的逗号是可选的。为了代码风格一致性，建议列表或字典等结构中，最后一行的元素后保持逗号，这样便于以后增加或调整内容\n",
        "    # RandomRotation: 随机旋转特征数据，最大旋转角度为 ±40% 的全角。\n",
        "    keras.layers.RandomRotation(0.4),\n",
        "\n",
        "    # TensorFlow 2.4 及以下\n",
        "    # keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\")\n",
        "    # keras.layers.experimental.preprocessing.RandomRotation(0.4)\n",
        "])\n",
        "\n",
        "# 2. 特征与标签的同步增强\n",
        "def augment_feature_label(feature, label):\n",
        "    \"\"\"\n",
        "    对单个特征和标签进行同步数据增强。\n",
        "    \"\"\"\n",
        "    # features 的形状是 (num_samples, 11, 5, 5)\n",
        "    # labels 的形状是 (num_samples,)\n",
        "    # 对于特征数据，需要将 (11, 5, 5) 转换为 (5, 5, 11) 以便进行数据增强操作。\n",
        "    # 增强后，再将其转换回原来的格式 (11, 5, 5)。\n",
        "\n",
        "    #  1. Keras 数据增强层要求输入形状为 (H, W, C)，特征 feature 的形状从 (11, 5, 5) 转换为 (5, 5, 11)。\n",
        "    feature = tf.transpose(feature, perm=[1, 2, 0])  # (11, 5, 5) -> (5, 5, 11)\n",
        "\n",
        "    # 2. 将标签广播成与特征匹配的形状\n",
        "    # 先将标签扩展到 (5, 5) 的二维张量，再扩展到 (5, 5, 1)\n",
        "    # feature[..., 0]或者feature[:, :, 0]： 提取了第一个特征通道的二维切片，其形状为 (5, 5)。\n",
        "    # tf.ones_like(feature[:, :, 0]) 生成了一个与这个切片形状相同的张量，值全为 1，形状为 (5, 5)。\n",
        "    # label * tf.ones_like(feature[:, :, 0]) 将标量 label（形状为 ()）扩展为一个形状为 (5, 5) 的二维张量，每个元素的值都等于 label。\n",
        "    # tf.expand_dims(..., axis=-1)在最后一个维度上为标签添加一个新维度。目的：将标签的形状与特征数据的通道维度对齐（即从二维变为三维）。\n",
        "    expanded_label = tf.expand_dims(label * tf.ones_like(feature[..., 0]), axis=-1)  # (5, 5, 1)\n",
        "\n",
        "    # 3. 将特征和标签沿通道维度拼接\n",
        "    # 将特征和标签组合为一个四维张量 combined，使得增强操作能够同步作用在特征和标签上。\n",
        "    # feature 是增强前的特征数据，其形状为 (5, 5, 11)，表示 5×5 空间大小，11 个特征通道。\n",
        "    # tf.stack 将两个张量（特征数据和标签数据）沿新的维度进行堆叠。feature 的形状为 (5, 5, 11)。标签经过上述操作后，形状为 (5, 5, 1)。\n",
        "    # 堆叠后的 combined 张量形状为 (5, 5, 12)，表示 11 个特征通道 + 1 个标签通道。\n",
        "    combined = tf.concat([feature, expanded_label], axis=-1)  # (5, 5, 12)\n",
        "\n",
        "    # 4. 数据增强\n",
        "    # 使用 data_augmentation 对组合的张量进行数据增强，保证特征和标签同步增强。\n",
        "    # augmented 是增强后的张量，其形状为 (5, 5, 12)：第三个维度（最后一个维度）包含 12 个通道，前 11 个是增强后的特征数据，最后 1 个是增强后的标签数据。\n",
        "    augmented = data_augmentation(combined)\n",
        "\n",
        "    # 5. 分离增强后的特征和标签\n",
        "    # augmented_feature: 增强后的特征数据。\n",
        "    # augmented_label: 增强后的标签数据，提取了原始广播的标签值，仍保持不变。\n",
        "    # ... 是省略号，表示选取前面所有维度（这里是第 1 和第 2 维，即 (5, 5) 的空间维度）。\n",
        "    # :-1 表示选择最后一个维度（第 3 维）的前 11 个通道。具体来说：从第 0 通道到第 10 通道（不包括第 11 通道）。\n",
        "    # augmented_feature 的形状为 (5, 5, 11)，即增强后的特征数据。\n",
        "    augmented_feature = augmented[..., :-1]  # 取前 11 个通道 (5, 5, 11)\n",
        "    # ... 表示选取前面所有维度（这里是第 1 和第 2 维，即 (5, 5) 的空间维度）。\n",
        "    # -1 表示选择最后一个通道（第 11 通道），即标签通道。\n",
        "    # 选择标签通道后，形状为 (5, 5)。接下来的操作是从 (5, 5) 中提取一个标量标签：[0, 0] 表示取出标签通道的第 (0, 0) 位置的值。由于标签在增强过程中被广播为 (5, 5)，所以整个通道中的值都是一样的，选取任意一个值即可。这里选取了 (0, 0) 位置的值。\n",
        "    # augmented_label 是一个标量，表示增强后的标签值，形状为 ()。\n",
        "    augmented_label = augmented[..., -1, 0, 0]  # 提取标签，取第一个值即可（标量）\n",
        "\n",
        "    # 6. 恢复特征原始格式 (C, H, W)\n",
        "    # 将增强后的特征形状转换回原始格式 (11, 5, 5)\n",
        "    augmented_feature = tf.transpose(augmented_feature, perm=[2, 0, 1])  # (5, 5, 11) -> (11, 5, 5)\n",
        "\n",
        "    return augmented_feature, augmented_label\n",
        "\n",
        "# 3. 对所有样本进行数据增强\n",
        "augmented_features = []\n",
        "augmented_labels = []\n",
        "\n",
        "# 遍历每个样本，对每对 feature 和 label 调用 augment_feature_label 函数进行数据增强。\n",
        "for i in range(features.shape[0]):\n",
        "    feature = tf.convert_to_tensor(features[i], dtype=tf.float32)\n",
        "    label = tf.convert_to_tensor(labels[i], dtype=tf.float32)\n",
        "\n",
        "    aug_feature, aug_label = augment_feature_label(feature, label)\n",
        "    augmented_features.append(aug_feature)\n",
        "    augmented_labels.append(aug_label)\n",
        "\n",
        "# 转换为 NumPy 数组\n",
        "# 将增强后的特征列表转换为四维 NumPy 数组 (num_samples, 11, 5, 5)。\n",
        "augmented_features = np.stack(augmented_features)\n",
        "# 将增强后的标签列表转换为一维 NumPy 数组 (num_samples,)。\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "# 5. 打印验证形状\n",
        "print(\"增强后的特征形状:\", augmented_features.shape)  # (num_samples, 11, 5, 5)\n",
        "print(\"增强后的标签形状:\", augmented_labels.shape)  # (num_samples,)\n",
        "\n",
        "features = augmented_features\n",
        "labels = augmented_labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkJaXmFXrnxy",
        "outputId": "760a27b9-92f0-4c44-e310-ebce4f2a6218"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "增强后的特征形状: (71417, 11, 5, 5)\n",
            "增强后的标签形状: (71417,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据平衡"
      ],
      "metadata": {
        "id": "big7o7PWKg4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 每个类别的样本数，确保每个标签区间有 800 个样本\n",
        "# sample_size = 800 #every class with labels smaller 36 meters has over 800 values\n",
        "# #features = np.mean(features, axis=(2, 3)) # patch mean of size * size features\n",
        "\n",
        "# # 生成从 3 到 36 步长为 3 的数字列表，即 [3, 6, 9, ..., 36]\n",
        "# num = (list(range(3, 37, 3))) #create list from 3 to 36 step 3\n",
        "# # 假设每个样本是一个 5x5 的图像块（大小为 5x5，90 个通道）\n",
        "# shape = (11, 5, 5)\n",
        "# # 创建一个初始的数组用于存储特征数据，形状为 (90, 5, 5)\n",
        "# data_bal = np.ones(shape) #create array to fill with features\n",
        "# # 扩展维度，使得形状变为 (1, 11, 5, 5)，这样可以进行拼接\n",
        "# data_bal = np.expand_dims(data_bal, axis=0) #expand one dimension to concatenate\n",
        "# # 创建一个用于存储标签的初始数组，形状为 (1,)\n",
        "# data_lab = np.ones(1) #create array to fill labels\n",
        "\n",
        "# # 在抽样之前，打印每个区间的样本数量，确保逻辑合理。\n",
        "# # 遍历每个标签区间\n",
        "# for i in num:\n",
        "#   # 注意这个 i 是区间右端点\n",
        "#   # 从标签中选择属于当前区间的索引\n",
        "#   # np.where() 返回的是一个元组，元组的元素个数取决于判断条件中的数据的维度, 元组的每个元素都是一个 数组，这些数组表示满足条件的元素在原始数组中的索引。因此，需要通过 indices[0] 访问索引数组\n",
        "#   # 如果输入数组是 多维的，返回的元组会包含 每一维的索引数组。例如，若数组是三维的，返回的元组就会包含三个数组，分别表示满足条件的元素在三维空间中每一维的索引。\n",
        "#   indices = np.where((labels > i-3) & (labels <= i)) #select indcies from every 3 meter interval until 36\n",
        "#   print(f\"区间 ({i-3}, {i}] 的样本数: {len(indices[0])}\")\n",
        "\n",
        "#   # 根据样本数决定如何抽样\n",
        "#   if len(indices[0]) < sample_size:\n",
        "#       print(f\"样本不足800，仅有 {len(indices[0])} 个样本，允许重复抽样。\")\n",
        "#       sampled_indices = np.random.choice(indices[0], size=sample_size, replace=True)\n",
        "#   else:\n",
        "#       sampled_indices = np.random.choice(indices[0], size=sample_size, replace=False)\n",
        "\n",
        "\n",
        "#   # 在当前区间中随机抽样 800 个样本\n",
        "#   # sampled_indices = np.random.choice(indices[0].flatten(), size=sample_size, replace=False) #random sample of each interval\n",
        "#   # sampled_indices = np.random.choice(indices[0], size=sample_size, replace=False)\n",
        "\n",
        "#   # 提取对应的特征和标签\n",
        "#   tempx = features[sampled_indices]\n",
        "#   tempy = labels[sampled_indices]\n",
        "#   # 将当前区间的特征和标签拼接到平衡数组中\n",
        "#   data_bal = np.concatenate((data_bal, tempx), axis=0)\n",
        "#   data_lab = np.concatenate((data_lab, tempy), axis=0)\n",
        "\n",
        "# # 处理 labels > 36 的标签，这部分直接拼接\n",
        "# indices = np.where((labels > 36)) #add the values > 36 m, they are so few no sample needed\n",
        "# # sampled_indices = indices[0].flatten()\n",
        "# sampled_indices = indices[0]\n",
        "# tempx = features[sampled_indices]\n",
        "# tempy = labels[sampled_indices]\n",
        "# data_bal = np.concatenate((data_bal[1:], tempx), axis=0)\n",
        "# data_lab = np.concatenate((data_lab[1:], tempy), axis=0)\n",
        "\n",
        "# # # 为了配合可视化界面，这里将data_bal重新赋值给features将data_lab重新赋值给labels，以便后续能够使用统一的变量。\n",
        "# features = data_bal\n",
        "# labels = data_lab\n"
      ],
      "metadata": {
        "id": "Oq5-JDlxKceX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "qH6Ck7wc7MuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# features 数组形状为 (num_samples, 11, 5, 5)，意味着每个样本有 11 个特征（或 11 个通道），每个特征是一个 5x5 的空间窗口。\n",
        "features_mean = np.mean(features, axis=(2, 3)) # patch mean of size * size features\n",
        "\n",
        "# 现在 features_mean 的形状是 (num_samples, 11)，适用于NN或者其他传统机器学习算法\n",
        "# 注意: train_test_split 只能处理 NumPy 数组或 Pandas DataFrame，并不能直接处理 TensorFlow Dataset 对象。因此，这部分代码在处理 TensorFlow Dataset 时会出错。\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_mean, labels, test_size = 0.3, random_state=3)\n"
      ],
      "metadata": {
        "id": "0eSNeRy4568n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以根据统计信息来决定是否需要对特征进行标准化处理"
      ],
      "metadata": {
        "id": "pyD_bqRA7lwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印 features_mean 的统计信息\n",
        "print(\"features_mean 的统计信息：\")\n",
        "print(f\"最大值: {np.max(features_mean)}\")\n",
        "print(f\"最小值: {np.min(features_mean)}\")\n",
        "print(f\"均值: {np.mean(features_mean)}\")\n",
        "print(f\"标准差: {np.std(features_mean)}\")\n"
      ],
      "metadata": {
        "id": "ITD_RgPU7WlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d916944-2362-4503-f1c2-6806db59b20d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features_mean 的统计信息：\n",
            "最大值: 1780.56298828125\n",
            "最小值: -23.87438201904297\n",
            "均值: 44.14850997924805\n",
            "标准差: 111.40662384033203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 针对每个特征维度（即 11 个通道）计算统计信息\n",
        "for i in range(features_mean.shape[1]):  # 遍历 11 个特征通道\n",
        "    print(f\"特征通道 {i+1} 的统计信息：\")\n",
        "    print(f\"  最大值: {np.max(features_mean[:, i])}\")\n",
        "    print(f\"  最小值: {np.min(features_mean[:, i])}\")\n",
        "    print(f\"  均值: {np.mean(features_mean[:, i])}\")\n",
        "    print(f\"  标准差: {np.std(features_mean[:, i])}\")\n"
      ],
      "metadata": {
        "id": "6wWkZ-M67gCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250a3de9-2214-4609-b376-6d1efdfba9fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "特征通道 1 的统计信息：\n",
            "  最大值: 1780.56298828125\n",
            "  最小值: 0.3511420488357544\n",
            "  均值: 263.642333984375\n",
            "  标准差: 265.0421142578125\n",
            "特征通道 2 的统计信息：\n",
            "  最大值: 38.97483825683594\n",
            "  最小值: 0.0\n",
            "  均值: 21.82733726501465\n",
            "  标准差: 4.483627796173096\n",
            "特征通道 3 的统计信息：\n",
            "  最大值: 42.82842254638672\n",
            "  最小值: 0.0\n",
            "  均值: 25.16364860534668\n",
            "  标准差: 5.273593902587891\n",
            "特征通道 4 的统计信息：\n",
            "  最大值: 42.67560577392578\n",
            "  最小值: 0.0\n",
            "  均值: 25.164608001708984\n",
            "  标准差: 5.2916364669799805\n",
            "特征通道 5 的统计信息：\n",
            "  最大值: 0.9374183416366577\n",
            "  最小值: 0.0\n",
            "  均值: 0.7477226853370667\n",
            "  标准差: 0.13860896229743958\n",
            "特征通道 6 的统计信息：\n",
            "  最大值: 2.504796028137207\n",
            "  最小值: 0.0\n",
            "  均值: 0.1462147831916809\n",
            "  标准差: 0.09872875362634659\n",
            "特征通道 7 的统计信息：\n",
            "  最大值: 137.7657470703125\n",
            "  最小值: 0.0\n",
            "  均值: 22.970035552978516\n",
            "  标准差: 15.970948219299316\n",
            "特征通道 8 的统计信息：\n",
            "  最大值: 295.5982971191406\n",
            "  最小值: 0.0\n",
            "  均值: 54.888187408447266\n",
            "  标准差: 39.68488693237305\n",
            "特征通道 9 的统计信息：\n",
            "  最大值: 457.3380432128906\n",
            "  最小值: 0.0\n",
            "  均值: 83.5849838256836\n",
            "  标准差: 62.217952728271484\n",
            "特征通道 10 的统计信息：\n",
            "  最大值: 0.6731970310211182\n",
            "  最小值: -23.87438201904297\n",
            "  均值: -12.490633010864258\n",
            "  标准差: 2.7513537406921387\n",
            "特征通道 11 的统计信息：\n",
            "  最大值: 0.998349130153656\n",
            "  最小值: -0.9985151886940002\n",
            "  均值: -0.010996442288160324\n",
            "  标准差: 0.5582660436630249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在标准化特征时，使用 训练集 来拟合标准化器（如 StandardScaler），并用其参数（均值和标准差）对测试集进行变换，这是一种 防止数据泄露 的必要操作。"
      ],
      "metadata": {
        "id": "lv4JTont8YZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "标准化的正确流程：\n",
        "1. 用训练集计算标准化参数（如均值和标准差）。\n",
        "2. 用这些参数对训练集和测试集分别进行标准化。"
      ],
      "metadata": {
        "id": "OevE-1UY8cQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 初始化标准化器\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit_transform()：计算训练集的统计参数（均值和标准差）。根据这些参数对数据进行标准化。适用于训练集\n",
        "# 仅用训练集数据拟合标准化器（避免数据泄露）\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# transform()：不会重新计算统计参数，而是使用 fit_transform() 计算出的参数对数据进行标准化。适用于测试集（或者未来的预测数据）。\n",
        "# 使用训练集的标准化参数变换测试集\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "CyPjz0gX8XZd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "kC-g42jz8hW-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 神经网络模型（NN模型）定义"
      ],
      "metadata": {
        "id": "qY26zkSm8nLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 Sequential 模型，适用于线性堆叠的神经网络。\n",
        "modelNn = Sequential()  # build neural network\n",
        "\n",
        "# 第一层：Dense(128, input_shape=(11,), ...)代表输入层，输入形状为 11,)，表示每个输入样本有 11 个特征。这个 11 是硬编码的，但实际上它应该等于特征的维度数，这里可能需要根据实际情况进行修改。\n",
        "# 如果特征数是动态变化的，建议不要硬编码这个数字，而是通过 features.shape[1] 获取动态的特征维度。\n",
        "\n",
        "input_dim = features.shape[1]  # 动态获取特征维度\n",
        "modelNn.add(Dense(128, input_shape=(input_dim,), kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# 第二层和第三层：分别包含 256 个神经元，也是全连接层，使用 ReLU 激活函数。\n",
        "modelNn.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "modelNn.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "modelNn.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# 使用 Dropout(0.4) 防止过拟合，随机丢弃 40% 的神经元\n",
        "modelNn.add(Dropout(0.4))\n",
        "\n",
        "# 输出层包含 1 个神经元，activation='linear' 表示线性激活函数，适用于回归任务。\n",
        "modelNn.add(Dense(1, kernel_initializer='normal', activation='linear'))\n",
        "\n",
        "\n",
        "# 打印模型的结构，包括每一层的类型、输出形状、参数数量等信息，帮助你了解模型的结构。\n",
        "modelNn.summary()\n"
      ],
      "metadata": {
        "id": "7TG9BKhk8ju6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "d867eea8-b289-4df4-fb5b-2450da42ad65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m1,536\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m33,024\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m133,377\u001b[0m (521.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">133,377</span> (521.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m133,377\u001b[0m (521.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">133,377</span> (521.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyK0QPxn8OeH",
        "outputId": "a172ead2-8fea-4f8a-de99-01a245c4f943"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 Google Colab 中运行代码时，如果你需要使用 TensorFlow Addons 提供的 CyclicalLearningRate，你需要先安装 TensorFlow Addons，因为它并不是 TensorFlow 的默认组件。但是TensorFlow Addons 支持的 TensorFlow 版本范围是 2.13.0 到 2.15.x，而你当前的 TensorFlow 版本是 2.17.1，超出了兼容范围。\n",
        "\n",
        "所以我使用Keras 内置的学习率调度器来代替 TensorFlow Addons。"
      ],
      "metadata": {
        "id": "Rt1_Xxj69C8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 卸载当前版本的 TensorFlow：\n",
        "# !pip uninstall -y tensorflow\n",
        "# # 安装兼容版本的 TensorFlow：\n",
        "# !pip install tensorflow==2.15\n",
        "# # 重新安装 TensorFlow Addons（如果需要）:\n",
        "# !pip install tensorflow-addons\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "om-rtPeo88Dt",
        "outputId": "5241a747-a31c-4386-d7eb-8032795c626a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.17.1\n",
            "Uninstalling tensorflow-2.17.1:\n",
            "  Successfully uninstalled tensorflow-2.17.1\n",
            "Collecting tensorflow==2.15\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (24.12.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.69.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.0\n",
            "    Uninstalling wrapt-1.17.0:\n",
            "      Successfully uninstalled wrapt-1.17.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.71 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow",
                  "wrapt"
                ]
              },
              "id": "0c9cc56013b445288cea7c851498abba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "定义带有 CLR 的 Adam **优化器**\n",
        "(在这里不使用)"
      ],
      "metadata": {
        "id": "QoCAH2BzYZMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "def cyclical_lr(epoch, lr):\n",
        "    # 最小学习率 (1e-5)，学习率波动的下限值。\n",
        "    min_lr = 1e-5\n",
        "    # maximal_learning_rate: 最大学习率 (1e-3)，学习率波动的上限值。\n",
        "    # max_lr = 1e-3 是一个安全的起点，但不是最优解。\n",
        "    # 如果怀疑学习率过小，可以逐渐增大 max_lr，或直接使用学习率范围测试找到最佳学习率范围。\n",
        "    # 对于绝大多数使用 Adam 优化器的中型或大型网络，1e-3 到 5e-3 通常是有效的范围。\n",
        "    max_lr = 1e-3\n",
        "    # step_size：步长，即学习率从最小值到最大值需要的 epoch 数。一个完整周期需要 2 × step_size 个 epoch。\n",
        "    step_size = 2000\n",
        "    # cycle 表示当前处于第几个学习率周期。\n",
        "      # np.floor(1 + ...) 将其向下取整，确保从第 1 个周期开始。\n",
        "      # epoch / (2 * step_size) 计算当前 epoch 在总周期中的位置。\n",
        "    cycle = np.floor(1 + epoch / (2 * step_size))\n",
        "    # x 计算当前 epoch 在周期中的相对位置，范围在 [0, 1]。\n",
        "      # 当 x 接近 0，学习率接近最大值。\n",
        "      # 当 x 接近 1，学习率接近最小值。\n",
        "    x = np.abs(epoch / step_size - 2 * cycle + 1)\n",
        "    new_lr = min_lr + (max_lr - min_lr) * max(0, (1 - x)) / (2 ** (cycle - 1))\n",
        "    return new_lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(cyclical_lr)\n",
        "# modelNn.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), callbacks=[lr_scheduler, early_stopping])\n",
        "\n",
        "\n",
        "# from tensorflow_addons.optimizers import CyclicalLearningRate\n",
        "# clr = CyclicalLearningRate(\n",
        "#     # 最小学习率 (1e-5)，学习率波动的下限值。\n",
        "#     initial_learning_rate=1e-5,\n",
        "#     # maximal_learning_rate: 最大学习率 (1e-3)，学习率波动的上限值。\n",
        "#     maximal_learning_rate=1e-3,\n",
        "#     # 步长，设置学习率从最小到最大需要的更新步数 (2000)，一个完整的学习率周期需要 2 * step_size 个更新步数。\n",
        "#     step_size=2000,\n",
        "#     # scale_fn: 缩放函数，定义学习率波动的模式。这里的 lambda x: 1/(2.**(x-1)) 是一个衰减函数，使得每个周期的学习率范围逐渐减小。\n",
        "#     scale_fn=lambda x: 1/(2.**(x-1)),\n",
        "#     # scale_mode: 缩放模式。'cycle' 表示在每个周期结束时应用 scale_fn，使学习率周期性衰减。\n",
        "#     scale_mode='cycle'\n",
        "# )\n",
        "# # 定义带有 CLR 的 Adam 优化器\n",
        "# # 将 CyclicalLearningRate (CLR) 应用于 Adam 优化器\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=clr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "H0RsaOcZYXkd",
        "outputId": "93a31894-2073-4828-d6cb-26c773544517"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.17.1 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.src.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f4ff51cba8d5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCyclicalLearningRate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m clr = CyclicalLearningRate(\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# 最小学习率 (1e-5)，学习率波动的下限值。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 损失函数：loss='mean_absolute_error' 使用绝对误差作为回归问题的损失函数。\n",
        "# 优化器：optimizer='adam' 使用 Adam 优化器，它是目前常用的一种高效的优化算法。\n",
        "# 评估指标：metrics=['mean_absolute_percentage_error'] 使用平均绝对百分比误差（MAPE）作为评估指标。\n",
        "\n",
        "# 注意: 在训练过程中，优化算法会根据 val_loss 来更新模型的参数，因为 val_loss 是损失函数的值，而损失函数通常是模型优化的目标。\n",
        "#    MAPE 作为评估指标，不会直接影响模型的参数更新，它仅用于评价模型在验证集上的相对误差，帮助你了解模型的实际表现。\n",
        "\n",
        "# 编译模型\n",
        "modelNn.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_percentage_error']) #compile model\n",
        "# modelNn.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['mean_absolute_percentage_error'])\n"
      ],
      "metadata": {
        "id": "XRSIGjCG86Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成一个包含当前时间戳的日志目录路径。为 TensorBoard 准备日志文件存储位置。\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))  # log directory for tensorboard\n",
        "\n",
        "# 在训练过程中将日志信息保存到指定的 logdir 目录，并设置每个 epoch 保存权重的直方图。在训练过程中启用 TensorBoard 回调，确保训练日志被记录。\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# 模型保存, 仅保存表现最好的模型。\n",
        "# HDF5 格式的模型文件，请确保扩展名为 .h5 或 .hdf5\n",
        "os.makedirs(\"/content/drive/My Drive/forest_height/models/NNmodels\", exist_ok=True)\n",
        "\n",
        "model_save = ModelCheckpoint(\n",
        "    \"/content/drive/My Drive/forest_height/models/NNmodels/best_NNmodel_Std.keras\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False  # 保存完整模型（包括架构、权重和优化器状态）\n",
        ")\n",
        "\n",
        "# 使用 Cyclical Learning Rate (CLR) 后删除 ReduceLROnPlateau，但建议保留 EarlyStopping\n",
        "\n",
        "# # NN模型和CNN模型可以分别定义不同的 ReduceLROnPlateau 回调函数，它们彼此独立，不会互相干扰。\n",
        "# reduce_lr_nn = ReduceLROnPlateau(\n",
        "#     monitor='val_loss',  # 监控验证集的损失\n",
        "#     factor=0.1,          # 学习率降低的比例为 20%\n",
        "#     patience=8,          # 等待 8 个 epoch 后再降低学习率\n",
        "#     verbose=1,           # 输出日志\n",
        "#     min_lr=1e-6          # 学习率下限\n",
        "# )\n",
        "\n",
        "\n",
        "# 如果 ReduceLROnPlateau 的 patience=5，建议 EarlyStopping 的 patience 设置为 2 * ReduceLROnPlateau 的 patience，即 10 或更高。\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # 监控验证集损失\n",
        "    patience=40,         # 在连续 30 个 epoch 验证集损失无改善时停止训练\n",
        "    restore_best_weights=True,  # 恢复至验证集损失最低(性能最佳)时的模型权重\n",
        "    verbose=1            # 输出早停信息\n",
        ")\n"
      ],
      "metadata": {
        "id": "e-AS9nOw8_Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 model.fit() 训练模型：\n",
        "# modelNn.fit(X_train, y_train, epochs = 100, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, model_save]) #fit model\n",
        "modelNn.fit(X_train, y_train, epochs = 200, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, model_save, early_stopping, lr_scheduler]) #fit model\n"
      ],
      "metadata": {
        "id": "6GQCRdpx9J1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras 中的函数，用于加载深度学习模型。\n",
        "bmodel = load_model('/content/drive/My Drive/forest_height/models/NNmodels/best_NNmodel_Std.keras') #load best model"
      ],
      "metadata": {
        "id": "QlaCS8V99QcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用训练好的模型对测试集 X_test 进行预测，返回预测值 ypred_nn。\n",
        "ypred_nn = bmodel.predict(X_test)\n"
      ],
      "metadata": {
        "id": "GcRg2ddz9U_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "\n",
        "# 计算测试集的均方误差。 (MSE)\n",
        "mse_nn = mse(y_test, ypred_nn)\n",
        "# 计算均方根误差 (RMSE)\n",
        "rmse_nn = mse_nn ** (1/2)\n",
        "# 计算平均绝对误差 (MAE)\n",
        "mae_nn = mae(y_test, ypred_nn)\n",
        "# 平均绝对百分比误差 (MAPE)\n",
        "mape_nn = mape(y_test, ypred_nn)\n",
        "# R²\n",
        "r2 = r2_score(y_test, ypred_nn)\n",
        "\n",
        "# 打印出 MAPE、MAE 和 RMSE 评估指标，帮助你评估模型的表现\n",
        "print(mape_nn)\n",
        "print(mae_nn)\n",
        "print(rmse_nn)\n",
        "print(r2)"
      ],
      "metadata": {
        "id": "IQYvhYgs9VlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "2J0nE48b9XyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算预测结果的均值\n",
        "mean_nn = np.mean(ypred_nn[:])  # calculate mean\n",
        "# 计算预测结果的不同分位数（1%, 25%, 50%, 75%, 99%）\n",
        "quantiles_nn = np.percentile(ypred_nn[:], [1, 25, 50, 75, 99])  # calculate quantiles 0.01, 0.25, 0.5, 0.75, 0.99\n",
        "\n",
        "# 计算标签的均值\n",
        "mean_labels = np.mean(labels[:])\n",
        "# 计算标签的不同分位数（1%, 25%, 50%, 75%, 99%）\n",
        "quantiles_labels = np.percentile(labels[:], [1, 25, 50, 75, 99])\n",
        "\n",
        "print(mean_nn)\n",
        "print(quantiles_nn)\n",
        "# 打印预测值中最小的 10 个和最大的 10 个。\n",
        "print(np.sort(ypred_nn.flatten())[:10])  # print the 10 lowest predictions\n",
        "print(np.sort(ypred_nn.flatten())[-10:][::-1])  # print the 10 highest predictions\n",
        "\n",
        "print(mean_labels)\n",
        "print(quantiles_labels)\n",
        "# 打印标签中最小的 10 个和最大的 10 个。\n",
        "print(np.sort(labels.flatten())[:10])\n",
        "print(np.sort(labels.flatten())[-10:][::-1])\n"
      ],
      "metadata": {
        "id": "aLkhRG8n9Zcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional neural network"
      ],
      "metadata": {
        "id": "D50-StiF9fwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据集拆分：训练集和测试集\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state=3) #create train, test set\n"
      ],
      "metadata": {
        "id": "Mp9sfsYr9hAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载 TensorBoard 插件，用于可视化训练过程的日志信息，如损失曲线、指标等。\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "PmkFLTtr9k6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 转换 X_train 和 X_test 的格式为 NHWC\n",
        "# y_train 和 y_test 是目标标签，它们通常是数值（对于回归问题）或分类标签（对于分类问题），是一维数组。目标标签与 CNN 的数据格式无关，因此无需调整。\n",
        "X_train = X_train.transpose(0, 2, 3, 1)  # 从 (N, C, H, W) 转换为 (N, H, W, C)\n",
        "X_test = X_test.transpose(0, 2, 3, 1)\n",
        "\n",
        "# 检查转换后的形状\n",
        "print(X_train.shape)  # 应输出 (49991, 5, 5, 11)\n",
        "print(X_test.shape)   # 应输出 (21426, 5, 5, 11)\n"
      ],
      "metadata": {
        "id": "p3grYI7I9xxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import BatchNormalization"
      ],
      "metadata": {
        "id": "blc5RKuxmkAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras 默认使用 channels_last 格式，这表示图像的输入形状是 (height, width, channels)。如果想使用 channels_first 格式（即 (channels, height, width)），需要进行相应的设置，比如通过 Keras 配置全局设置或者在模型层中明确指定\n",
        "# 这里采用为每个层指定数据格式。只有卷积层和池化层需要明确指定数据格式（channels_first 或 channels_last），而全连接层不需要这样做，展平层会处理掉格式问题。\n",
        "# 卷积操作会根据指定的数据格式进行计算，输出的形状也会遵循这种数据格式。\n",
        "\n",
        "# 使用 Sequential() 创建一个顺序模型（即按顺序堆叠各层）。\n",
        "modelCnn = Sequential() #bulid cnn\n",
        "\n",
        "# 因为 features 数组形状为 (num_samples, 5, 5, 11)\n",
        "# 指定输入数据的形状。这里假设每个输入图像是一个 11 个通道的 5x5 大小的图像块。\n",
        "modelCnn.add(InputLayer(input_shape=(5, 5, 11)))  # NHWC 格式 # 输入形状为 (height, width，channels)\n",
        "\n",
        "# 添加标准化层\n",
        "# BatchNormalization 是一个内置的标准化层，它会动态地对输入数据进行标准化处理。\n",
        "# 它会根据输入数据的批量大小，计算批量内每个特征的均值和标准差，对数据进行标准化。\n",
        "# 如果你已经在数据预处理步骤中手动对特征数据进行了标准化（如使用 StandardScaler），那么不建议再使用 BatchNormalization，以免标准化重复，影响模型的学习。\n",
        "# 如果你选择使用 BatchNormalization，可以跳过数据预处理中的手动标准化部分。\n",
        "modelCnn.add(BatchNormalization(axis=-1))  # 对每个通道进行标准化\n",
        "\n",
        "# 注意：在卷积神经网络中，卷积核的形状是 (高度, 宽度, 输入通道数, 输出通道数)，这代表每个卷积核的尺寸、它需要处理的输入通道数，以及它生成的输出通道数。\n",
        "# 高度和宽度决定了卷积核的空间大小，常见的卷积核大小有 3x3、5x5、7x7 等。\n",
        "# 输入通道数是卷积核需要处理的输入特征图的深度（即输入图像的通道数）。\n",
        "# 输出通道数是卷积层的过滤器数目，也就是卷积层最终生成的特征图的数量。\n",
        "\n",
        "# 当你定义一个卷积层时，卷积核的数量会决定卷积层输出的通道数，但卷积核的深度（即每个卷积核的输入通道数）是由输入数据的通道数（即输入特征图的深度）来决定的，而不用手动设置。\n",
        "# Conv2D 添加一个卷积层，filters=128 表示该层将有 128 个卷积核（即输出通道数），每个卷积核对应一个输出通道。\n",
        "# kernel_size=(3,3) 表示卷积核的大小是 3x3。trides=1 表示步幅为 1，即卷积核每次移动 1 个像素。padding=\"same\" 表示零填充（zero padding）策略，目的是使得 卷积操作后 输出特征图的尺寸 保持与输入相同（即宽度和高度保持不变）。activation='relu' 使用 ReLU 激活函数。\n",
        "# 第一层卷积核的形状是 (3, 3, 11，128)，该卷积层的输出图像的形状为(128, 5, 5)，其中 128 是输出的通道数，由于设置了padding = \"same\"，故输出图像大小不变。\n",
        "modelCnn.add(Conv2D(filters=128, kernel_size= (3,3), strides=  1 , padding = \"same\", activation='relu'))\n",
        "\n",
        "# 另一个卷积层，filters=256 代表使用 256 个卷积核（即输出通道数），每个卷积核对应一个输出通道。padding=\"valid\" 表示不使用填充，输出大小会减少。\n",
        "# 第二层卷积核的形状是 (3, 3, 128，256)，该卷积层的输出图像的形状为(256, 3, 3)，其中 256 是输出的通道数，由于使用了 valid padding，输出尺寸减少。\n",
        "modelCnn.add(Conv2D(filters=256, kernel_size= (3,3), strides=  1 , padding = \"valid\", activation='relu'))\n",
        "\n",
        "# MaxPool2D 添加一个最大池化层，pool_size=(2,2) 表示 2x2 的池化窗口，池化操作的 步长 默认是 2，减少空间维度（降低特征图的尺寸）。\n",
        "# 用 2x2 的过滤器，以2为步长进行特征值提取，因此该池化操作会将输入特征图的 空间尺寸（宽度和高度） 缩小一半。采用的池化操作是 最大池化。\n",
        "# 输入尺寸为 (256, 3, 3)，经过池化后，输出的尺寸会变为 (256, 1, 1)，因为池化操作会将 3x3 的特征图缩小为 1x1。\n",
        "modelCnn.add(MaxPool2D(pool_size = (2,2)))\n",
        "\n",
        "# Flatten() 层将二维的特征图展平为一维向量，为全连接层做准备。\n",
        "# Flatten() 会将池化层输出的 (256, 1, 1) 转换为 256 的一维向量。这是因为在全连接层之前，必须将输入转换为一维数据。\n",
        "modelCnn.add(Flatten())\n",
        "\n",
        "# 全连接层处理的是一维向量，而不是多维的图像数据。所以全连接层的输入格式不依赖于 channels_first 或 channels_last。\n",
        "# Dense(512) 添加一个全连接层，包含 512 个神经元，activation='relu' 使用 ReLU 激活函数。\n",
        "modelCnn.add(Dense(512, activation='relu'))\n",
        "\n",
        "# 另一个全连接层，包含 128 个神经元。\n",
        "modelCnn.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Dropout(0.4) 是一个正则化方法，随机丢弃 40% 的神经元，防止过拟合。\n",
        "modelCnn.add(Dropout(0.4))\n",
        "\n",
        "# 最后一层是一个包含 1 个神经元的全连接层，使用线性激活函数（适合回归任务）。\n",
        "modelCnn.add(Dense(1, activation='linear'))\n",
        "\n",
        "\n",
        "# summary() 输出模型的概况，显示每一层的参数数量和输出形状。\n",
        "modelCnn.summary()"
      ],
      "metadata": {
        "id": "yqdCLDGt92vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 compile() 方法指定模型的损失函数、优化器和评估指标。\n",
        "# 使用平均绝对误差作为损失函数. 使用 Adam 优化器. 使用平均绝对百分比误差作为评估指标。\n",
        "modelCnn.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_percentage_error']) #compile cnn\n",
        "# modelCnn.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['mean_absolute_percentage_error'])\n"
      ],
      "metadata": {
        "id": "xadltJsv-GZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置日志文件夹路径。\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "# TensorBoard 回调用于在训练过程中记录日志，供 TensorBoard 使用。\n",
        "# histogram_freq=1 表示每个 epoch 都记录权重的直方图\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "# ModelCheckpoint 回调用于在验证集损失最小化时保存最佳模型。\n",
        "# HDF5 格式的模型文件，请确保扩展名为 .h5 或 .hdf5\n",
        "os.makedirs(\"/content/drive/My Drive/forest_height/models/CNNmodels\", exist_ok=True)\n",
        "model_save = ModelCheckpoint(\"/content/drive/My Drive/forest_height/models/CNNmodels/best_CNNmodel_Std.keras\",\n",
        "                             save_best_only=True,\n",
        "                             save_weights_only=False)  # directory for best model\n",
        "\n",
        "# # 创建一个 ReduceLROnPlateau 回调实例，并设置其参数\n",
        "# reduce_lr_cnn = ReduceLROnPlateau(\n",
        "#     monitor='val_loss',  # 监控验证集的损失\n",
        "#     factor=0.2,          # 学习率降低的比例为 10%\n",
        "#     patience=10,         # 等待 10 个 epoch 后再降低学习率\n",
        "#     verbose=1,           # 输出日志\n",
        "#     min_lr=1e-6          # 学习率下限\n",
        "# )\n"
      ],
      "metadata": {
        "id": "yOidqDVC-Kfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 指定训练数据、测试数据、训练轮次（epochs=100）以及回调函数（tensorboard_callback 和 model_save）。\n",
        "# modelCnn.fit(X_train, y_train, epochs = 100, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, model_save]) #train cnn\n",
        "modelCnn.fit(X_train, y_train, epochs = 200, validation_data=(X_test, y_test), callbacks=[tensorboard_callback, model_save, early_stopping, lr_scheduler])\n"
      ],
      "metadata": {
        "id": "73Za4p2_-QfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bmodel = load_model('/content/drive/My Drive/forest_height/models/CNNmodels/best_CNNmodel_Std.keras')"
      ],
      "metadata": {
        "id": "fUxYISpe-Q_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred_cnn = bmodel.predict(X_test) #predict best model"
      ],
      "metadata": {
        "id": "aRjfquRD-VZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算均方误差（MSE）、均方根误差（RMSE）、平均绝对误差（MAE）和平均绝对百分比误差（MAPE）。\n",
        "# 这些指标用于评估模型在测试集上的性能。\n",
        "from sklearn.metrics import mean_absolute_percentage_error as mape, r2_score\n",
        "\n",
        "# 计算 MAPE\n",
        "mape_cnn = mape(y_test, y_pred_cnn)\n",
        "# 计算 R²\n",
        "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
        "\n",
        "print(\"MAPE:\", mape_cnn)\n",
        "print(\"R²:\", r2_cnn)\n",
        "\n",
        "mse_cnn = mse(y_test, y_pred_cnn) #calculate metrics\n",
        "rmse_cnn = mse_cnn ** (1/2)\n",
        "mae_cnn = mae(y_test, y_pred_cnn)\n",
        "mape_cnn = mape(y_test, y_pred_cnn)\n",
        "# R²\n",
        "r2_cnn = r2_score(y_test, y_pred_cnn)\n",
        "\n",
        "print(mape_cnn)\n",
        "print(mae_cnn)\n",
        "print(rmse_cnn)\n",
        "print(r2_cnn)"
      ],
      "metadata": {
        "id": "s6GI3n_6-WFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 启动 TensorBoard 可视化工具，查看训练过程中的详细信息，如损失曲线、准确率曲线、权重分布等。\n",
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "pdRlGEdT-YKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ypred_cnn[:] 和 labels[:] 都是对数组的切片操作，表示返回数组中的所有元素，分别是预测值和真实标签值。\n",
        "# 无论数组的维度是多少，使用 [:] 都是返回数组中的所有元素。这是 NumPy 中的一个常见用法，它用于获取数组的完整内容，不论数组是多维的。(保持其原有的形状。)\n",
        "\n",
        "# 计算预测结果的均值\n",
        "# ypred_cnn 的形状应为 (n, 1)，即一个列向量，每一行代表一个样本的预测结果。如果你打印 ypred_cnn[:]，你将得到模型对所有测试样本的预测结果。\n",
        "mean_cnn = np.mean(y_pred_cnn[:]) #calculate mean\n",
        "# 计算预测结果的分位数\n",
        "quantiles_cnn = np.percentile(y_pred_cnn[:], [1, 25, 50, 75, 99]) #calculate quantiles 0.01, 0.25, 0.5, 0.75, 0.99\n",
        "\n",
        "# 计算标签的均值\n",
        "# labels 是一个形状为 (n,) 的一维数组，存储了对应测试样本的实际标签（真实的森林高度）。\n",
        "# labels[:] 会返回所有的标签值。\n",
        "mean_labels = np.mean(labels[:])\n",
        "# 计算标签的分位数\n",
        "quantiles_labels = np.percentile(labels[:], [1, 25, 50, 75, 99])\n",
        "\n",
        "print(mean_cnn)\n",
        "print(quantiles_cnn)\n",
        "# 打印预测值中最小的 10 个和最大的 10 个。\n",
        "print(np.sort(y_pred_cnn.flatten())[:10]) #print the 10 lowest predictions\n",
        "print(np.sort(y_pred_cnn.flatten())[-10:][::-1]) #print the 10 highest predictions\n",
        "\n",
        "print(mean_labels)\n",
        "print(quantiles_labels)\n",
        "# 打印标签中最小的 10 个和最大的 10 个。\n",
        "print(np.sort(labels.flatten())[:10])\n",
        "print(np.sort(labels.flatten())[-10:][::-1])"
      ],
      "metadata": {
        "id": "8UxLzttK-YW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 保存带有输出的 notebook 文件到 Google Drive\n",
        "!jupyter nbconvert --to pdf \"/content/drive/My Drive/Colab Notebooks/NNandCNN_FeatureNormalization.ipynb\"\n"
      ],
      "metadata": {
        "id": "LDhRE8D43uM7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}